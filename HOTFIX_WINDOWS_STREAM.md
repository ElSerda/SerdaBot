# ğŸ”§ HOTFIX Windows - Phrases coupÃ©es (avant stream)

## ğŸ¯ Objectif
VÃ©rifier que les phrases ne sont plus coupÃ©es en plein milieu avec `max_tokens_chill=60`

## âœ… Ce qui a Ã©tÃ© fait sur Linux
- âœ… Tests locaux avec `test_max_tokens_60.py` â†’ OK
- âœ… Config modifiÃ© : `max_tokens_chill: 60` (Ã©tait 45)
- âœ… Commit + push sur GitHub

## ğŸªŸ Ã€ FAIRE SUR WINDOWS (machine de production)

### Ã‰tape 1 : Mettre Ã  jour le code
```powershell
cd C:\Users\YourUser\SerdaBot
git pull origin main
```

### Ã‰tape 2 : Mettre Ã  jour config.yaml
Ouvrir `src\config\config.yaml` et **vÃ©rifier** que ces lignes existent :
```yaml
bot:
  max_tokens_ask: 120       # Max tokens pour ASK (dÃ©taillÃ©)
  max_tokens_chill: 60      # Max tokens pour CHILL (conversationnel, Ã©tait 45)
  temperature_ask: 0.4      # Temperature ASK (factuel)
  temperature_chill: 0.7    # Temperature CHILL (crÃ©atif)
```

**Si ces lignes sont absentes**, les ajouter aprÃ¨s la ligne `model_timeout: 10`

### Ã‰tape 3 : Test rapide (optionnel mais recommandÃ©)
**Option A : Test manuel sur Twitch**
1. Lancer le bot : `.\start_bot.ps1`
2. Dans le chat Twitch, taper : `serda_bot que peux tu me dire sur nintendo ?`
3. âœ… VÃ©rifier que la phrase se termine correctement (pas coupÃ©e comme "...Super Mario et The Legend of")

**Option B : Test local (si tu veux Ãªtre sÃ»r)**
```powershell
# Activer venv
.\venv\Scripts\Activate.ps1

# Lancer le test
python scripts\test_max_tokens_60.py
```

### Ã‰tape 4 : RedÃ©marrer le bot pour le stream
```powershell
.\start_bot.ps1
```

## ğŸ“Š RÃ©sultats attendus

### âŒ AVANT (max_tokens=45)
```
"Nintendo est une sociÃ©tÃ© japonaise... leurs jeux comme Super Mario et The Legend of"
```
â˜ï¸ Phrase coupÃ©e net

### âœ… APRÃˆS (max_tokens=60)
```
"Nintendo est une grande sociÃ©tÃ© qui fait des jeux vidÃ©o et des consoles. Ils sont connus pour leurs consoles Nintendo 3DS, Nintendo Switch et leurs jeux comme Super Mario et The Legend of Zelda."
```
â˜ï¸ Phrase complÃ¨te avec point final

## âš ï¸ Note importante
- Le fichier `config.yaml` est **local** (pas dans git car contient des tokens)
- Le fichier `config.sample.yaml` est **le template** qui a Ã©tÃ© mis Ã  jour
- Si tu as besoin de reset ta config, copie `config.sample.yaml` â†’ `config.yaml` et rempli tes tokens

## ğŸš¨ ProblÃ¨me ? 
Si les phrases sont encore coupÃ©es :
1. VÃ©rifier que LM Studio utilise bien **Qwen2.5-3B-Instruct**
2. VÃ©rifier dans les logs : `[METRICS] ğŸ“¤ OUTPUT: X chars, ~Y tokens` (devrait Ãªtre ~60 tokens max)
3. Augmenter `max_tokens_chill` Ã  80 si nÃ©cessaire (mais teste d'abord 60)

## â° Timeline
- **Maintenant** : Tests locaux Linux âœ…
- **Avant le stream (dans ~19h)** : Tests Windows
- **Stream dimanche soir** : Bot live chez l'amie

Bon stream ! ğŸš€
