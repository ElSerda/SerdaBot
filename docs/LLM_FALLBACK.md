# ğŸ¤– SystÃ¨me de Fallback LLM

Ce document explique comment SerdaBot gÃ¨re l'absence de LLM local et garantit une expÃ©rience utilisateur cohÃ©rente.

---

## ğŸ¯ Philosophie

**ProblÃ¨me** : Un LLM local (LM Studio) n'est pas toujours disponible :
- âš ï¸ En CI (GitHub Actions) â†’ pas de GPU, pas de LM Studio
- âš ï¸ Sur un fork â†’ utilisateur sans LLM installÃ©
- âš ï¸ En production â†’ LM Studio peut crasher temporairement

**Solution** : SystÃ¨me de fallback automatique avec rÃ©ponses prÃ©-dÃ©finies.

**RÃ©sultat** :
- âœ… Bot **toujours fonctionnel** (avec ou sans LLM)
- âœ… Tests CI **100% verts** (pas de dÃ©pendance au LLM)
- âœ… Fork **immÃ©diatement utilisable** (pas besoin de GPU)
- âœ… Production **robuste** (pas de freeze si LLM down)

---

## ğŸ—ï¸ Architecture

### 1. DÃ©tection au dÃ©marrage

Au lancement du bot, `TwitchBot.__init__()` vÃ©rifie automatiquement la disponibilitÃ© du LLM :

```python
from src.utils.llm_detector import check_llm_status, get_llm_mode

# DÃ©tection du LLM
llm_mode = get_llm_mode(self.config)

if llm_mode == "auto":
    self.llm_available, status_msg = check_llm_status(self.config)
    print(status_msg)
    # â†’ "âœ… LLM dÃ©tectÃ© : http://localhost:1234/v1"
    # â†’ "âš ï¸  LLM non disponible â†’ mode fallback activÃ©"
```

**Avantages** :
- âœ… VÃ©rification **une seule fois** au boot (pas Ã  chaque commande)
- âœ… **0 latence** sur les commandes (dÃ©cision dÃ©jÃ  prise)
- âœ… DÃ©tection **silencieuse** (pas de spam rÃ©seau)

### 2. SystÃ¨me de fallback en cascade (NEW!)

**3 niveaux de fallback automatiques** :

```
Commande !ask
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£ LM Studio (local)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ âŒ Timeout/Error
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£ OpenAI API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ âŒ No API key / Error
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£ RÃ©pliques fun    â”‚  â† TOUJOURS disponible
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code interne** (`call_model()`) :
```python
# Niveau 1 : Essayer LM Studio
result = await try_endpoint(api_url, ...)
if result:
    return result  # âœ… SuccÃ¨s

# Niveau 2 : Essayer OpenAI
result = await try_openai_fallback(...)
if result:
    return result  # âœ… SuccÃ¨s

# Niveau 3 : Retourner None
return None  # âŒ Tous Ã©chouÃ©s â†’ commandes utilisent fallback rÃ©pliques
```

**Avantages** :
- âœ… **Robustesse maximale** : 3 niveaux de sÃ©curitÃ©
- âœ… **CoÃ»t optimisÃ©** : OpenAI seulement si LM Studio fail
- âœ… **ExpÃ©rience cohÃ©rente** : Bot toujours rÃ©actif
- âœ… **Production-ready** : Crash LM Studio = 0 downtime

### 3. SystÃ¨me de rÃ©ponses fallback

Quand le LLM n'est pas disponible, `src/core/fallbacks.py` fournit des rÃ©ponses prÃ©-dÃ©finies :

```python
from src.core.fallbacks import get_fallback_response

# RÃ©ponse pour !ask
response = get_fallback_response("ask")
# â†’ "Je rÃ©flÃ©chisâ€¦ mais mon cerveau est en pause ğŸ§ ğŸ’¤"

# RÃ©ponse pour @mention
response = get_fallback_response("chill")
# â†’ "Salut ! ğŸ‘‹"
```

**Intentions disponibles** :
- `"ask"` : RÃ©ponses pour `!ask` (8 variantes)
- `"chill"` : RÃ©ponses pour `@mention` (19 variantes)
- `"ask_timeout"` : Timeout LLM (4 variantes)
- `"ask_error"` : Erreur LLM (3 variantes) â† **UtilisÃ© quand tous les LLM Ã©chouent**

### 3. IntÃ©gration dans les commandes

Les commandes `!ask` et `!chill` gÃ¨rent automatiquement le fallback en cascade :

```python
async def handle_ask_command(..., llm_available: bool = True):
    # Boot detection : Si LLM marquÃ© indisponible au dÃ©marrage â†’ fallback immÃ©diat
    if not llm_available:
        fallback_msg = get_fallback_response("ask")
        await message.channel.send(f"@{user} {fallback_msg}")
        return
    
    # Runtime cascade : Essayer LM Studio â†’ OpenAI â†’ Fallback rÃ©pliques
    response = await call_model(prompt, config, user=user, mode="ask")
    
    # Si tous les LLM ont Ã©chouÃ© (retourne None) â†’ fallback rÃ©pliques
    if response is None:
        fallback_msg = get_fallback_response("ask_error")
        await message.channel.send(f"@{user} {fallback_msg}")
        return
    
    # Sinon â†’ utiliser la rÃ©ponse LLM normale
    await message.channel.send(f"@{user} {response}")
```

**Double sÃ©curitÃ©** :
- âœ… **Boot detection** : Si aucun LLM au dÃ©marrage â†’ skip appel rÃ©seau
- âœ… **Runtime cascade** : Si crash post-boot â†’ fallback automatique

---

## ğŸ”§ Configuration

### Mode automatique (par dÃ©faut)

```yaml
# Pas de config nÃ©cessaire â†’ auto-dÃ©tection
```

Le bot dÃ©tecte automatiquement si LM Studio est accessible.

### Mode manuel (optionnel)

Tu peux forcer l'activation/dÃ©sactivation via variable d'environnement :

```bash
# Forcer l'utilisation du LLM (mÃªme si non dÃ©tectÃ©)
export LLM_MODE=enabled
./start_bot.sh

# Forcer le mode fallback (mÃªme si LLM disponible)
export LLM_MODE=disabled
./start_bot.sh

# Auto-dÃ©tection (dÃ©faut)
export LLM_MODE=auto
./start_bot.sh
```

Ou via config (future feature) :

```yaml
bot:
  llm:
    enabled: auto  # auto | true | false
    endpoint: "http://localhost:1234/v1"
    fallback_mode: "fun"  # fun | silent | minimal
```

**Modes fallback** :
- `fun` : RÃ©pliques humoristiques (dÃ©faut)
- `silent` : Message neutre ("Commande temporairement indisponible.")
- `minimal` : Juste un emoji ("ğŸ¤–")

---

## ğŸ§ª Tests

### Tests avec LLM (marquÃ©s `@pytest.mark.llm`)

Ces tests **nÃ©cessitent LM Studio** et sont **skippÃ©s en CI** :

```python
import pytest

@pytest.mark.llm
@pytest.mark.asyncio
async def test_ask_with_real_llm():
    """Teste que le LLM rÃ©pond correctement (nÃ©cessite LM Studio)."""
    response = await call_model("C'est quoi Python ?", config, mode='ask')
    assert "langage" in response.lower()
```

### Tests sans LLM (toujours exÃ©cutÃ©s)

Ces tests vÃ©rifient le **fallback** et passent **toujours** :

```python
async def test_ask_fallback_when_no_llm():
    """Teste le fallback quand LLM indisponible."""
    response = get_fallback_response("ask")
    assert len(response) > 5
    assert len(response) < 200
```

### Lancer les tests

```bash
# Par dÃ©faut : skip les tests LLM
pytest tests/

# Lancer TOUS les tests (y compris LLM)
pytest tests/ -m ""

# Lancer UNIQUEMENT les tests LLM
pytest tests/ -m "llm"

# Lancer tout SAUF les tests LLM
pytest tests/ -m "not llm"
```

---

## ğŸ“Š Flux de dÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bot dÃ©marre           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ©tection LLM           â”‚
â”‚ (llm_detector.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â”‚           â”‚
      â–¼           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ LLM OK â”‚  â”‚ Pas de â”‚
 â”‚        â”‚  â”‚  LLM   â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚          â”‚
      â”‚          â”‚
      â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Commande â”‚ â”‚ Commande â”‚
â”‚ !ask     â”‚ â”‚ !ask     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cascade  â”‚ â”‚ Fallback â”‚
â”‚ LMâ†’OpenAIâ”‚ â”‚ direct   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â–¼            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚LLM OK?â”‚   â”‚ RÃ©plique â”‚
 â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚   fun    â”‚
     â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”Œâ”€â”€â”€â”´â”€â”€â”€â”
 â”‚Fail?  â”‚
 â””â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚
     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ RÃ©plique â”‚
 â”‚   fun    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ Personnalisation des rÃ©ponses

### Ajouter des rÃ©ponses custom

```python
from src.core.fallbacks import add_custom_fallback

# Ajouter des rÃ©ponses pour une nouvelle intention
add_custom_fallback("greetings", [
    "Coucou ! ğŸ‘‹",
    "Salut la compagnie !",
    "Bien ou bien ? ğŸ˜"
])

# Utiliser
response = get_fallback_response("greetings")
```

### Modifier les rÃ©ponses existantes

Ã‰dite `src/core/fallbacks.py` :

```python
FALLBACKS = {
    "ask": [
        "Ton message custom ici ğŸ¤–",
        # ... autres rÃ©ponses
    ],
    "chill": [
        "Ta salutation custom ! ğŸ‘‹",
        # ... autres rÃ©ponses
    ],
}
```

---

## ğŸš€ Pour les contributeurs

### Ajouter un nouveau fallback

1. **Ajoute l'intention dans `fallbacks.py`** :

```python
FALLBACKS = {
    # ... intentions existantes
    "nouvelle_intention": [
        "RÃ©ponse 1",
        "RÃ©ponse 2",
        "RÃ©ponse 3",
    ],
}
```

2. **Utilise dans ta commande** :

```python
if not llm_available:
    response = get_fallback_response("nouvelle_intention")
    await message.channel.send(response)
    return
```

3. **Ajoute des tests** :

```python
def test_nouvelle_intention():
    response = get_fallback_response("nouvelle_intention")
    assert isinstance(response, str)
    assert len(response) > 0
```

### Guidelines

- âœ… RÃ©ponses **courtes** (< 200 chars)
- âœ… Ton **cohÃ©rent** avec le bot (geek, fun, second degrÃ©)
- âœ… Plusieurs **variantes** (Ã©viter la rÃ©pÃ©tition)
- âœ… **Twitch-safe** (< 500 chars absolument)

---

## ğŸ› Troubleshooting

### Le bot utilise toujours le fallback (alors que LLM est dispo)

**Cause** : LM Studio non dÃ©tectÃ© au dÃ©marrage.

**Solution** :
```bash
# VÃ©rifie que LM Studio est bien lancÃ©
curl http://localhost:1234/v1/models

# Si erreur â†’ lance LM Studio
# Sinon â†’ redÃ©marre le bot
```

### Le bot utilise le LLM (alors que je veux le fallback)

**Solution** :
```bash
# Force le mode fallback
export LLM_MODE=disabled
./start_bot.sh
```

### Le bot crashe si LM Studio tombe en plein live

**RÃ©ponse** : âœ… **Non, il ne crashe pas !** Avec le systÃ¨me de cascade :

```
1. User: !ask Question
2. Bot essaie LM Studio â†’ timeout (10s max)
3. Bot essaie OpenAI â†’ fail (pas de clÃ©)
4. Bot envoie rÃ©plique fallback â†’ "ğŸ’€ Mes neurones bugguent, rÃ©essaie dans 2 min !"
```

**RÃ©sultat** : 0 downtime, expÃ©rience cohÃ©rente ! ğŸ¯

### Les tests LLM Ã©chouent en local

**Cause** : LM Studio non lancÃ©.

**Solution** :
```bash
# Option 1 : Lance LM Studio
# Option 2 : Skip les tests LLM
pytest tests/ -m "not llm"
```

### Comment savoir quel niveau de fallback a Ã©tÃ© utilisÃ© ?

**Logs** :
```
[MODEL] ğŸ”— Tentative LM Studio...
[MODEL] âŒ LM_STUDIO failed: timeout
[MODEL] ğŸŒ Utilisation du fallback OpenAI (si configurÃ©)
[MODEL] âš ï¸ Pas de clÃ© OpenAI configurÃ©e
[ASK] ğŸ¤– Tous LLM indisponibles â†’ fallback rÃ©pliques
[ASK] âœ… Fallback error envoyÃ©: ğŸ’€ Mes neurones bugguent...
```

---

## ğŸ“š RÃ©fÃ©rences

- **Code source** :
  - `src/utils/llm_detector.py` - DÃ©tection LLM
  - `src/core/fallbacks.py` - RÃ©ponses fallback
  - `src/chat/twitch_bot.py` - IntÃ©gration au bot

- **Tests** :
  - `tests/test_llm_fallback.py` - Tests du systÃ¨me fallback
  - `tests/test_pipeline_integration.py` - Tests marquÃ©s `@pytest.mark.llm`

- **Config** :
  - `pytest.ini` - Configuration markers pytest
  - `src/config/config.example.yaml` - Template de config

---

## â“ FAQ

**Q: Le fallback ralentit-il les commandes ?**  
R: Non, la dÃ©tection est faite **une seule fois au boot**. Sur les commandes, c'est juste un `if` â†’ **0 latence**.

**Q: Peut-on utiliser le bot UNIQUEMENT en mode fallback ?**  
R: Oui ! `export LLM_MODE=disabled` â†’ bot 100% fonctionnel sans LLM.

**Q: Les rÃ©ponses fallback sont-elles toujours les mÃªmes ?**  
R: Non, elles sont **alÃ©atoires** (choix random dans la liste).

**Q: Peut-on mÃ©langer LLM + fallback ?**  
R: Oui ! Le bot peut passer de l'un Ã  l'autre (ex: LLM crash en live â†’ fallback immÃ©diat).

**Q: Faut-il modifier le code pour ajouter des rÃ©ponses ?**  
R: Non, utilise `add_custom_fallback()` en runtime ou Ã©dite `fallbacks.py`.

**Q: Que se passe-t-il si LM Studio crash pendant un live ?**  
R: Cascade automatique : LM Studio (fail) â†’ OpenAI (si configurÃ©) â†’ RÃ©pliques fun. **0 downtime** ! ğŸ¯

**Q: Combien coÃ»te l'utilisation d'OpenAI en fallback ?**  
R: Seulement si LM Studio Ã©choue. Avec `gpt-4o-mini` : ~0.0001$ par rÃ©ponse (nÃ©gligeable).

**Q: Peut-on dÃ©sactiver OpenAI et garder seulement LM Studio + RÃ©pliques ?**  
R: Oui ! Laisse la section `openai` vide dans `config.yaml` â†’ cascade LM Studio â†’ RÃ©pliques directement.

---

## ğŸ–ï¸ CrÃ©dits

SystÃ¨me conÃ§u pour garantir une expÃ©rience utilisateur cohÃ©rente, que le bot ait accÃ¨s Ã  un LLM ou non.

**Philosophie** : Un bon bot Twitch doit **toujours rÃ©pondre**, jamais rester silencieux. ğŸ¯
