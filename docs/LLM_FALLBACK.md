# ğŸ¤– SystÃ¨me de Fallback LLM

Ce document explique comment SerdaBot gÃ¨re l'absence de LLM local et garantit une expÃ©rience utilisateur cohÃ©rente.

---

## ğŸ¯ Philosophie

**ProblÃ¨me** : Un LLM local (LM Studio) n'est pas toujours disponible :
- âš ï¸ En CI (GitHub Actions) â†’ pas de GPU, pas de LM Studio
- âš ï¸ Sur un fork â†’ utilisateur sans LLM installÃ©
- âš ï¸ En production â†’ LM Studio peut crasher temporairement

**Solution** : SystÃ¨me de fallback automatique avec rÃ©ponses prÃ©-dÃ©finies.

**RÃ©sultat** :
- âœ… Bot **toujours fonctionnel** (avec ou sans LLM)
- âœ… Tests CI **100% verts** (pas de dÃ©pendance au LLM)
- âœ… Fork **immÃ©diatement utilisable** (pas besoin de GPU)
- âœ… Production **robuste** (pas de freeze si LLM down)

---

## ğŸ—ï¸ Architecture

### 1. DÃ©tection au dÃ©marrage

Au lancement du bot, `TwitchBot.__init__()` vÃ©rifie automatiquement la disponibilitÃ© du LLM :

```python
from src.utils.llm_detector import check_llm_status, get_llm_mode

# DÃ©tection du LLM
llm_mode = get_llm_mode(self.config)

if llm_mode == "auto":
    self.llm_available, status_msg = check_llm_status(self.config)
    print(status_msg)
    # â†’ "âœ… LLM dÃ©tectÃ© : http://localhost:1234/v1"
    # â†’ "âš ï¸  LLM non disponible â†’ mode fallback activÃ©"
```

**Avantages** :
- âœ… VÃ©rification **une seule fois** au boot (pas Ã  chaque commande)
- âœ… **0 latence** sur les commandes (dÃ©cision dÃ©jÃ  prise)
- âœ… DÃ©tection **silencieuse** (pas de spam rÃ©seau)

### 2. SystÃ¨me de rÃ©ponses fallback

Quand le LLM n'est pas disponible, `src/core/fallbacks.py` fournit des rÃ©ponses prÃ©-dÃ©finies :

```python
from src.core.fallbacks import get_fallback_response

# RÃ©ponse pour !ask
response = get_fallback_response("ask")
# â†’ "Je rÃ©flÃ©chisâ€¦ mais mon cerveau est en pause ğŸ§ ğŸ’¤"

# RÃ©ponse pour @mention
response = get_fallback_response("chill")
# â†’ "Salut ! ğŸ‘‹"
```

**Intentions disponibles** :
- `"ask"` : RÃ©ponses pour `!ask` (8 variantes)
- `"chill"` : RÃ©ponses pour `@mention` (19 variantes)
- `"ask_timeout"` : Timeout LLM (4 variantes)
- `"ask_error"` : Erreur LLM (3 variantes)

### 3. IntÃ©gration dans les commandes

Les commandes `!ask` et `!chill` vÃ©rifient automatiquement `llm_available` :

```python
async def handle_ask_command(..., llm_available: bool = True):
    # Si LLM indisponible â†’ fallback immÃ©diat
    if not llm_available:
        fallback_msg = get_fallback_response("ask")
        await message.channel.send(f"@{user} {fallback_msg}")
        return
    
    # Sinon â†’ logique LLM normale
    response = await call_model(...)
```

---

## ğŸ”§ Configuration

### Mode automatique (par dÃ©faut)

```yaml
# Pas de config nÃ©cessaire â†’ auto-dÃ©tection
```

Le bot dÃ©tecte automatiquement si LM Studio est accessible.

### Mode manuel (optionnel)

Tu peux forcer l'activation/dÃ©sactivation via variable d'environnement :

```bash
# Forcer l'utilisation du LLM (mÃªme si non dÃ©tectÃ©)
export LLM_MODE=enabled
./start_bot.sh

# Forcer le mode fallback (mÃªme si LLM disponible)
export LLM_MODE=disabled
./start_bot.sh

# Auto-dÃ©tection (dÃ©faut)
export LLM_MODE=auto
./start_bot.sh
```

Ou via config (future feature) :

```yaml
bot:
  llm:
    enabled: auto  # auto | true | false
    endpoint: "http://localhost:1234/v1"
    fallback_mode: "fun"  # fun | silent | minimal
```

**Modes fallback** :
- `fun` : RÃ©pliques humoristiques (dÃ©faut)
- `silent` : Message neutre ("Commande temporairement indisponible.")
- `minimal` : Juste un emoji ("ğŸ¤–")

---

## ğŸ§ª Tests

### Tests avec LLM (marquÃ©s `@pytest.mark.llm`)

Ces tests **nÃ©cessitent LM Studio** et sont **skippÃ©s en CI** :

```python
import pytest

@pytest.mark.llm
@pytest.mark.asyncio
async def test_ask_with_real_llm():
    """Teste que le LLM rÃ©pond correctement (nÃ©cessite LM Studio)."""
    response = await call_model("C'est quoi Python ?", config, mode='ask')
    assert "langage" in response.lower()
```

### Tests sans LLM (toujours exÃ©cutÃ©s)

Ces tests vÃ©rifient le **fallback** et passent **toujours** :

```python
async def test_ask_fallback_when_no_llm():
    """Teste le fallback quand LLM indisponible."""
    response = get_fallback_response("ask")
    assert len(response) > 5
    assert len(response) < 200
```

### Lancer les tests

```bash
# Par dÃ©faut : skip les tests LLM
pytest tests/

# Lancer TOUS les tests (y compris LLM)
pytest tests/ -m ""

# Lancer UNIQUEMENT les tests LLM
pytest tests/ -m "llm"

# Lancer tout SAUF les tests LLM
pytest tests/ -m "not llm"
```

---

## ğŸ“Š Flux de dÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bot dÃ©marre           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ©tection LLM           â”‚
â”‚ (llm_detector.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â”‚           â”‚
      â–¼           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ LLM OK â”‚  â”‚ Pas de â”‚
 â”‚        â”‚  â”‚  LLM   â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚          â”‚
      â”‚          â”‚
      â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Commande â”‚ â”‚ Commande â”‚
â”‚ !ask     â”‚ â”‚ !ask     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Appel    â”‚ â”‚ Fallback â”‚
â”‚ LLM      â”‚ â”‚ prÃ©-def  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ Personnalisation des rÃ©ponses

### Ajouter des rÃ©ponses custom

```python
from src.core.fallbacks import add_custom_fallback

# Ajouter des rÃ©ponses pour une nouvelle intention
add_custom_fallback("greetings", [
    "Coucou ! ğŸ‘‹",
    "Salut la compagnie !",
    "Bien ou bien ? ğŸ˜"
])

# Utiliser
response = get_fallback_response("greetings")
```

### Modifier les rÃ©ponses existantes

Ã‰dite `src/core/fallbacks.py` :

```python
FALLBACKS = {
    "ask": [
        "Ton message custom ici ğŸ¤–",
        # ... autres rÃ©ponses
    ],
    "chill": [
        "Ta salutation custom ! ğŸ‘‹",
        # ... autres rÃ©ponses
    ],
}
```

---

## ğŸš€ Pour les contributeurs

### Ajouter un nouveau fallback

1. **Ajoute l'intention dans `fallbacks.py`** :

```python
FALLBACKS = {
    # ... intentions existantes
    "nouvelle_intention": [
        "RÃ©ponse 1",
        "RÃ©ponse 2",
        "RÃ©ponse 3",
    ],
}
```

2. **Utilise dans ta commande** :

```python
if not llm_available:
    response = get_fallback_response("nouvelle_intention")
    await message.channel.send(response)
    return
```

3. **Ajoute des tests** :

```python
def test_nouvelle_intention():
    response = get_fallback_response("nouvelle_intention")
    assert isinstance(response, str)
    assert len(response) > 0
```

### Guidelines

- âœ… RÃ©ponses **courtes** (< 200 chars)
- âœ… Ton **cohÃ©rent** avec le bot (geek, fun, second degrÃ©)
- âœ… Plusieurs **variantes** (Ã©viter la rÃ©pÃ©tition)
- âœ… **Twitch-safe** (< 500 chars absolument)

---

## ğŸ› Troubleshooting

### Le bot utilise toujours le fallback (alors que LLM est dispo)

**Cause** : LM Studio non dÃ©tectÃ© au dÃ©marrage.

**Solution** :
```bash
# VÃ©rifie que LM Studio est bien lancÃ©
curl http://localhost:1234/v1/models

# Si erreur â†’ lance LM Studio
# Sinon â†’ redÃ©marre le bot
```

### Le bot utilise le LLM (alors que je veux le fallback)

**Solution** :
```bash
# Force le mode fallback
export LLM_MODE=disabled
./start_bot.sh
```

### Les tests LLM Ã©chouent en local

**Cause** : LM Studio non lancÃ©.

**Solution** :
```bash
# Option 1 : Lance LM Studio
# Option 2 : Skip les tests LLM
pytest tests/ -m "not llm"
```

---

## ğŸ“š RÃ©fÃ©rences

- **Code source** :
  - `src/utils/llm_detector.py` - DÃ©tection LLM
  - `src/core/fallbacks.py` - RÃ©ponses fallback
  - `src/chat/twitch_bot.py` - IntÃ©gration au bot

- **Tests** :
  - `tests/test_llm_fallback.py` - Tests du systÃ¨me fallback
  - `tests/test_pipeline_integration.py` - Tests marquÃ©s `@pytest.mark.llm`

- **Config** :
  - `pytest.ini` - Configuration markers pytest
  - `src/config/config.example.yaml` - Template de config

---

## â“ FAQ

**Q: Le fallback ralentit-il les commandes ?**  
R: Non, la dÃ©tection est faite **une seule fois au boot**. Sur les commandes, c'est juste un `if` â†’ **0 latence**.

**Q: Peut-on utiliser le bot UNIQUEMENT en mode fallback ?**  
R: Oui ! `export LLM_MODE=disabled` â†’ bot 100% fonctionnel sans LLM.

**Q: Les rÃ©ponses fallback sont-elles toujours les mÃªmes ?**  
R: Non, elles sont **alÃ©atoires** (choix random dans la liste).

**Q: Peut-on mÃ©langer LLM + fallback ?**  
R: Oui ! Le bot peut passer de l'un Ã  l'autre (ex: LLM crash en live â†’ fallback immÃ©diat).

**Q: Faut-il modifier le code pour ajouter des rÃ©ponses ?**  
R: Non, utilise `add_custom_fallback()` en runtime ou Ã©dite `fallbacks.py`.

---

## ğŸ–ï¸ CrÃ©dits

SystÃ¨me conÃ§u pour garantir une expÃ©rience utilisateur cohÃ©rente, que le bot ait accÃ¨s Ã  un LLM ou non.

**Philosophie** : Un bon bot Twitch doit **toujours rÃ©pondre**, jamais rester silencieux. ğŸ¯
