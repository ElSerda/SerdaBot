# ðŸš€ Guide Rapide - SerdaBot Production

Guide rapide pour dÃ©marrer et maintenir SerdaBot en production avec Qwen 2.5-1.5B.

---

## âš¡ DÃ©marrage Rapide

### 1. PrÃ©requis

- **Python 3.10+**
- **LM Studio** installÃ© et lancÃ©
- **Qwen2.5-1.5B-Instruct-Q4_K_M.gguf** chargÃ© dans LM Studio
- Tokens Twitch configurÃ©s dans `src/config/config.yaml`

### 2. Installation

```bash
# Clone le repo
git clone https://github.com/ElSerda/SerdaBot.git
cd SerdaBot

# Installe les dÃ©pendances
pip install -r requirements.txt

# Configure les secrets (copie template)
cp src/config/config.yaml.example src/config/config.yaml
nano src/config/config.yaml  # Ajoute tes tokens Twitch/OpenAI
```

### 3. VÃ©rification LM Studio

```bash
# VÃ©rifie que LM Studio tourne
curl http://127.0.0.1:1234/v1/models

# Test rapide modÃ¨le
python scripts/smoke_model.py
```

### 4. Lancement

```bash
# Linux/macOS
bash start_bot.sh

# Windows
.\start_bot.ps1
```

---

## ðŸ¤– ModÃ¨le Production

**Qwen 2.5-1.5B-Instruct-Q4_K_M** (optimal ratio perf/ressources)

### Config LM Studio
- **Endpoint**: `http://127.0.0.1:1234/v1/chat/completions`
- **Laisse LM Studio gÃ©rer**: GPU layers, context size, etc.
- **Pas besoin ajuster**: Le bot envoie tempÃ©ratures/tokens optimaux

### Performances ValidÃ©es
- âœ… 93% succÃ¨s ASK (â‰¤250 chars)
- âœ… 80% succÃ¨s CHILL (1-5 mots)
- âœ… 0.1-0.4s latence (acceptable live)
- âœ… 0 hallucinations (85 cas testÃ©s)

ðŸ“– **DÃ©tails**: Voir `docs/MODEL_CONFIG.md`

---

## ðŸŽ® Commandes Twitch

| Commande | Usage | Exemple |
|----------|-------|---------|
| `!ask <question>` | Question factuelle | `!ask c'est quoi python ?` |
| `@serda_bot <message>` | Interaction fun/cool | `@serda_bot salut !` â†’ "Yo." |
| `!game <titre>` | Info jeu vidÃ©o | `!game Elden Ring` |
| `!trad <texte>` | Traduction FR | `!trad hello world` |

### Mode ASK
- **But**: RÃ©ponses concises et claires
- **Limite**: 200 chars prompt â†’ rÃ©el â‰¤250 chars
- **Exemples**:
  - "python" â†’ "Langage populaire pour scripts et IA."
  - "blockchain" â†’ "Technologie de registre dÃ©centralisÃ© sÃ©curisÃ©."

### Mode CHILL
- **But**: RÃ©ponses ultra-courtes fun/cool
- **Limite**: 1-5 mots maximum
- **Exemples**:
  - "lol" â†’ "Marrant."
  - "t'es qui toi ?" â†’ "Le bot du stream."
  - "bravo" â†’ "Incroyable."

---

## ðŸ”§ Configuration

### Fichiers ClÃ©s

```
src/
  config/
    config.yaml          # SECRETS (tokens Twitch/OpenAI) - IGNORÃ‰ GIT âš ï¸
  prompts/
    prompt_loader.py     # Prompts ASK/CHILL optimaux + few-shot
  utils/
    model_utils.py       # API calls LM Studio (config dynamique)
  chat/
    twitch_bot.py        # Bot Twitch principal
```

### Variables Importantes

**config.yaml** (secrets):
```yaml
bot:
  channel: ton_channel_twitch
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions
  openai_model: "gpt-4o-mini"  # Fallback si LM Studio down (100% succÃ¨s, 4x moins cher)
  cooldown: 10  # Secondes entre rÃ©ponses

twitch:
  token: oauth:xxxxx
  client_id: xxxxx
  client_secret: xxxxx

openai:
  api_key: sk-proj-xxxxx  # Fallback GPT-4o-mini si LM Studio down
```

**prompt_loader.py** (prompts):
```python
SYSTEM_ASK_FINAL = """Maximum 200 caractÃ¨res par rÃ©ponse."""
SYSTEM_CHILL_FINAL = """1-5 mots maximum. Style dÃ©contractÃ©."""
```

**model_utils.py** (config API):
```python
# ASK: dÃ©terministe, explications complÃ¨tes
temperature = 0.4
max_tokens = 80
stop = ["\n\n"]

# CHILL: stable, ultra court
temperature = 0.5
max_tokens = 20
stop = None
```

---

## ðŸ§ª Tests

### Test Rapide (15 cas)
```bash
python personnal/tests/test_ask_200_chars.py
```

### Test Massif (85 cas)
```bash
python personnal/tests/test_production_final.py
```

### Test Live (smoke test)
```bash
# VÃ©rifie endpoint LM Studio
python scripts/smoke_model.py

# VÃ©rifie bot complet (sans Twitch)
python scripts/smoke_bot.py
```

---

## ðŸ› Troubleshooting

### Bot ne rÃ©pond pas

1. **VÃ©rifie LM Studio**:
   ```bash
   curl http://127.0.0.1:1234/v1/models
   ```
   Si erreur: Relance LM Studio et charge Qwen2.5-1.5B-Instruct-Q4_K_M

2. **VÃ©rifie logs**:
   ```bash
   tail -f logs/serdabot.log
   ```

3. **VÃ©rifie cooldown** (10s entre messages):
   - User doit attendre 10s entre commandes

### RÃ©ponses trop longues

1. **VÃ©rifie prompts** (`src/prompts/prompt_loader.py`):
   ```python
   SYSTEM_ASK_FINAL  # Doit contenir "200 caractÃ¨res"
   SYSTEM_CHILL_FINAL  # Doit contenir "1-5 mots"
   ```

2. **VÃ©rifie config** (`src/utils/model_utils.py`):
   ```python
   optimal_max_tokens = 80 if mode == "ask" else 20
   ```

3. **Re-run tests validation**:
   ```bash
   python personnal/tests/test_ask_200_chars.py
   ```

### RÃ©ponses chinoises

âŒ **RÃ©solu en production** (prompts franÃ§ais natifs + few-shot)

Si Ã§a arrive:
1. VÃ©rifie `SYSTEM_ASK_FINAL` et `SYSTEM_CHILL_FINAL` en franÃ§ais
2. VÃ©rifie few-shot enrichi activÃ© (mode chill)

### LM Studio down â†’ Fallback OpenAI

Le bot bascule automatiquement sur **GPT-4o-mini** si LM Studio indisponible.

**Logs attendus**:
```
[MODEL] âš ï¸ LM Studio indisponible
[MODEL] ðŸŒ Fallback OpenAI (GPT-4o-mini)...
```

**Performances fallback**:
- ASK: 100% â‰¤250 chars (188 chars moyenne, 1.52s)
- CHILL: 100% â‰¤5 mots (3.8 mots moyenne, 0.67s)
- CoÃ»t: 4x moins cher que GPT-3.5-turbo

---

## ðŸ“Š Monitoring

### MÃ©triques Logs

Le bot log automatiquement:
```
[METRICS] ðŸ“¥ INPUT: 45 chars, ~11 tokens
[METRICS] ðŸ“¤ OUTPUT: 28 chars, ~7 tokens
[METRICS] âš¡ DurÃ©e: 0.23s, 30.4 tok/s
```

### Performances Attendues

**ASK**:
- 93% rÃ©ponses â‰¤250 chars
- Moyenne: 136 chars
- Latence: ~0.4s

**CHILL**:
- 80% rÃ©ponses â‰¤5 mots
- Moyenne: 2.6 mots
- Latence: ~0.1s

**Alertes**:
- âš ï¸ Si latence >1s: Machine surchargÃ©e ou LM Studio lent
- âš ï¸ Si >500 chars: Prompt modifiÃ© par erreur

---

## ðŸ”„ Maintenance

### Mise Ã  jour modÃ¨le

Si upgrade hardware â†’ Qwen 2.5-7B (100% succÃ¨s validÃ©):
1. TÃ©lÃ©charge `Qwen2.5-7B-Instruct-Q4_K_M.gguf`
2. Charge dans LM Studio
3. Relance bot (config identique, performances meilleures)

### Backup config

**IMPORTANT**: `config.yaml` contient secrets, **pas committÃ©** Git âœ…

Backup manuel:
```bash
cp src/config/config.yaml src/config/config.yaml.backup
```

### Rollback version

Si problÃ¨me aprÃ¨s update:
```bash
git checkout v0.1.0-alpha  # DerniÃ¨re version stable
pip install -r requirements.txt
```

---

## ðŸ“š Documentation ComplÃ¨te

- **Model Config**: `docs/MODEL_CONFIG.md` (dÃ©tails techniques optimisation)
- **Tests Archive**: `personnal/tests/README.md` (historique 17 tests)
- **Changelog**: `CHANGELOG.md` (historique versions)
- **Commands**: `COMMANDS.md` (liste commandes complÃ¨tes)
- **Structure**: `PROJECT_STRUCTURE.md` (architecture code)

---

## ðŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/ElSerda/SerdaBot/issues)
- **Twitch**: [@El_Serda](https://twitch.tv/el_serda)
- **Docs**: `/docs` dans ce repo

---

## ðŸŽ¯ Checklist DÃ©marrage

- [ ] LM Studio installÃ© et lancÃ©
- [ ] Qwen2.5-1.5B-Instruct-Q4_K_M.gguf chargÃ©
- [ ] `config.yaml` configurÃ© (tokens Twitch)
- [ ] `pip install -r requirements.txt` OK
- [ ] `python scripts/smoke_model.py` OK
- [ ] `bash start_bot.sh` OK
- [ ] Bot rÃ©pond sur Twitch âœ…

**Si tout âœ… â†’ PrÃªt pour production ! ðŸŽ‰**
