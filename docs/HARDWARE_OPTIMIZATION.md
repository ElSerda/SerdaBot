# ğŸ”§ Hardware Optimization Guide

Guide d'optimisation du SerdaBot pour diffÃ©rentes configurations matÃ©rielles.

---

## ğŸ“Š Benchmarks de rÃ©fÃ©rence

### RTX 3080 (10GB VRAM) - Configuration de dÃ©veloppement

**Setup testÃ© :**
```yaml
max_tokens_chill: 100
temperature_chill: 0.8
max_messages_per_user: 12
model_timeout: 10s
KV offload: ON
GPU layers: ALL (33/33)
```

**RÃ©sultats (4 users Ã— 10 messages) :**
- â±ï¸ Latence moyenne : **980ms**
- ğŸš€ Throughput : **3.87 msg/s**
- ğŸ“ˆ Min/Max : 520ms / 1318ms
- âœ… Contexte max : ~180 tokens input

**Avec KV offload dÃ©sactivÃ© (âŒ NE PAS FAIRE) :**
- â±ï¸ Latence moyenne : **1763ms** (+80%)
- ğŸš€ Throughput : **2.16 msg/s** (-44%)
- âš ï¸ DÃ©gradation critique avec contexte croissant

---

## ğŸ¯ GTX 780 (3GB VRAM) + CPU Fallback - Configuration production

### âš™ï¸ Configuration recommandÃ©e

**LM Studio / llama.cpp :**
```
Model: Qwen2.5-3B-Instruct-Q4_K_M.gguf
GPU Layers: 20-25 (limite VRAM 3GB)
KV Cache: VRAM (CRITIQUE !)
Context: 2048 tokens
Flash Attention: OFF (pas supportÃ© sur Kepler)
```

**config.yaml :**
```yaml
bot:
  max_tokens_chill: 100        # Ã‰vite troncatures
  temperature_chill: 0.8       # DiversitÃ© rÃ©ponses
  model_timeout: 10            # GÃ©nÃ©reux pour CPU fallback

rate_limiting:
  max_concurrent_users: 4      # Limite charge parallÃ¨le
  max_messages_per_user: 12    # Contexte maÃ®trisÃ© (~180 tokens)
  max_idle_time: 3600          # Nettoyage mÃ©moire
```

### ğŸ“ˆ Performances attendues

**Avec KV offload ON (âœ… OBLIGATOIRE) :**
- Latence GPU (layers 0-20) : **3-5s**
- Latence CPU fallback : **4-8s**
- Throughput : **0.8-1.2 msg/s** (4 users)
- âœ… Dans le timeout de 10s

**Sans KV offload (ğŸ’€ MORT DU BOT) :**
- Latence : **6-10s** (recalcul complet Ã  chaque token)
- Risque timeout : **Ã©levÃ©**
- Utilisation CPU : **100%** (chauffe excessive)
- âŒ Non viable en production

### ğŸ§  Pourquoi KV offload est CRITIQUE

Le **KV cache** (Key-Value cache) stocke les embeddings des tokens du contexte :
- **Avec KV cache** : Le modÃ¨le rÃ©utilise les calculs prÃ©cÃ©dents
- **Sans KV cache** : Recalcul complet Ã  chaque nouveau token

**Impact sur GTX 780 :**
```
Contexte 180 tokens Ã— 4 users parallÃ¨les = 720 tokens Ã  traiter
Sans KV cache : 720 tokens recalculÃ©s Ã  CHAQUE rÃ©ponse = ğŸ’€
Avec KV cache : Seulement les nouveaux tokens = âœ…
```

**RÃ©partition mÃ©moire optimale :**
```
VRAM 3GB :
â”œâ”€ ModÃ¨le (layers 0-20) : ~2.2GB
â”œâ”€ KV cache (prioritaire) : ~0.6GB
â””â”€ Overhead systÃ¨me : ~0.2GB

Reste (layers 21-33) â†’ CPU (10700K)
```

---

## ğŸ’» CPU-Only Fallback (10700K sans GPU)

**Si GPU indisponible ou surchargÃ© :**

**Setup :**
```
GPU Layers: 0 (full CPU)
Threads: 8 (10700K = 8c/16t)
KV offload: N/A (tout en RAM)
```

**Performances attendues :**
- â±ï¸ Latence : **4-8s** (acceptable avec timeout 10s)
- ğŸš€ Throughput : **0.5-0.8 msg/s**
- ğŸ’¾ RAM : ~4GB (modÃ¨le + contexte)

**Optimisations CPU :**
- Utilise `Q4_K_M` (pas Q5 ou Q6 = trop lent)
- Active `mmap` (chargement lazy du modÃ¨le)
- Limite `n_threads` Ã  8 (hyperthreading inutile pour infÃ©rence)

---

## ğŸ”¥ Comparaison Hardware

| GPU | VRAM | Layers | Latence (avg) | Throughput | KV Offload | Production |
|-----|------|--------|---------------|------------|------------|------------|
| **RTX 3080** | 10GB | 33/33 | 980ms | 3.87 msg/s | ON | âœ… Excellent |
| **GTX 780** | 3GB | 20-25 | 3-5s | 0.8-1.2 msg/s | **ON (CRITIQUE)** | âœ… Viable |
| **CPU 10700K** | - | 0 | 4-8s | 0.5-0.8 msg/s | N/A | âš ï¸ Acceptable |
| **GTX 780** | 3GB | 20-25 | 6-10s | 0.3-0.5 msg/s | âŒ OFF | ğŸ’€ Non viable |

---

## ğŸ“ Checklist de dÃ©ploiement GTX 780

Avant de lancer le bot en prod sur matÃ©riel limitÃ© :

### âœ… Configuration LM Studio / llama.cpp
- [ ] Model : `Qwen2.5-3B-Instruct-Q4_K_M.gguf`
- [ ] GPU Layers : 20-25 (tester limite VRAM)
- [ ] **KV Cache Offload : ON** (CRITIQUE)
- [ ] Context : 2048 tokens
- [ ] Flash Attention : OFF

### âœ… Configuration bot (config.yaml)
- [ ] `max_tokens_chill: 100`
- [ ] `temperature_chill: 0.8`
- [ ] `model_timeout: 10`
- [ ] `max_concurrent_users: 4`
- [ ] `max_messages_per_user: 12`

### âœ… Tests de performance
```bash
# Test benchmark avec params production
python3 scripts/benchmark_conversation_manager.py --messages 10

# VÃ©rifier :
# - Latence moyenne < 6s (idÃ©al < 5s)
# - Aucun timeout (< 10s)
# - Historique limitÃ© Ã  12 messages
# - Pas de memory leak (plusieurs runs)
```

### âœ… Monitoring en prod
```bash
# Logue les latences Ã©levÃ©es
tail -f logs/bot.log | grep "â±ï¸"

# Surveille RAM/VRAM
watch -n 2 nvidia-smi
htop
```

---

## ğŸ¯ Optimisations avancÃ©es

### 1. Cache de rÃ©ponses frÃ©quentes

Le bot dispose dÃ©jÃ  d'un cache RAM pour les facts :
- Questions frÃ©quentes â†’ 0ms latence
- Ã‰conomise 90% de la charge sur "C'est quoi Python ?"

**Extension recommandÃ©e :**
```python
# Cache 10 questions les plus frÃ©quentes
FREQUENT_QA = {
    "c'est quoi python": "Python ? Le langage qui me permet d'exister ! ğŸ",
    "explique tuple": "Un tuple, c'est une liste qui refuse de changer ! ğŸ“¦",
    # ...
}
```

### 2. Fallback conversationnel

Si latence > 5s, envoyer un message temporaire :
```python
if latency > 5.0:
    await send("Mon cerveau compileâ€¦ RÃ©essaie dans 2 sec ? â³")
```

### 3. Diversification des prompts

Le modÃ¨le rÃ©pÃ¨te "Ã‰videmment, t'es pas la premiÃ¨reâ€¦" â†’ variantes :
```python
PROMPT_PREFIXES = [
    "Bien sÃ»r",
    "Ã‰videmment",
    "Sans problÃ¨me",
    "Laisse-moi t'expliquer"
]
prefix = random.choice(PROMPT_PREFIXES)
```

### 4. Post-processing des troncatures

Si code Python tronquÃ©, complÃ©ter automatiquement :
```python
def fix_truncated_code(response: str) -> str:
    if "print(" in response and not response.count("(") == response.count(")"):
        return response + ")"
    return response
```

---

## ğŸš¨ Troubleshooting

### ProblÃ¨me : Latence > 10s (timeouts frÃ©quents)

**Causes possibles :**
1. âŒ KV offload dÃ©sactivÃ© â†’ **ACTIVER IMMÃ‰DIATEMENT**
2. Trop de GPU layers â†’ RÃ©duire Ã  15-20
3. Contexte trop grand â†’ VÃ©rifier `max_messages_per_user`

**Solutions :**
```bash
# Test avec KV offload ON
# RÃ©duire GPU layers si OOM
# RÃ©duire max_messages Ã  8 si nÃ©cessaire
```

### ProblÃ¨me : RÃ©ponses tronquÃ©es

**Cause :** `max_tokens_chill` trop bas

**Solution :**
```yaml
max_tokens_chill: 100  # Ã©tait 60, augmentÃ©
```

### ProblÃ¨me : RÃ©ponses rÃ©pÃ©titives

**Cause :** `temperature` trop basse

**Solution :**
```yaml
temperature_chill: 0.8  # Ã©tait 0.7, augmentÃ©
```

### ProblÃ¨me : VRAM Out of Memory

**Cause :** Trop de GPU layers ou KV cache trop grand

**Solution :**
```
# RÃ©duire GPU layers progressivement
GPU Layers: 25 â†’ 20 â†’ 15
# Ou rÃ©duire contexte
max_messages_per_user: 12 â†’ 8
```

---

## ğŸ“š RÃ©fÃ©rences

- **Benchmark scripts :** `scripts/benchmark_conversation_manager.py`
- **Configuration :** `src/config/config.yaml`
- **ConversationManager :** `src/utils/conversation_manager.py`
- **Model utils :** `src/utils/model_utils.py`

---

## ğŸ‰ TL;DR

**Pour GTX 780 + CPU :**
1. âœ… KV offload **ON** (non nÃ©gociable)
2. âœ… max_tokens_chill = 100
3. âœ… GPU layers = 20-25
4. âœ… max_messages = 12
5. âœ… timeout = 10s

**RÃ©sultat attendu :** 3-5s latence, stable, production-ready sur hardware 2013 ğŸš€

---

*DerniÃ¨re mise Ã  jour : 2025-10-21*  
*TestÃ© avec : Qwen2.5-3B-Instruct-Q4_K_M, RTX 3080, GTX 780, i7-10700K*
