# ü§ñ Configuration Mod√®le - SerdaBot

## Mod√®le Production

**Qwen 2.5-1.5B-Instruct-Q4_K_M** (via LM Studio local)

### Pourquoi ce mod√®le ?

- ‚úÖ **Performances valid√©es**: 93% succ√®s ASK + 80% succ√®s CHILL (objectif: 80%)
- ‚úÖ **Latence optimale**: 0.1-0.4s (acceptable pour chat Twitch en direct)
- ‚úÖ **Z√©ro hallucinations**: Tests massifs 85 cas r√©els (0 erreur)
- ‚úÖ **Hardware friendly**: Fonctionne sur petites machines (vs Qwen 7B trop lourd)
- ‚úÖ **100% limite IRC**: Toutes r√©ponses <500 chars (limite Twitch)

### Mod√®les test√©s (rejet√©s)

| Mod√®le | Taille | Succ√®s | Latence | Verdict |
|--------|--------|--------|---------|---------|
| Phi-2 Q4/Q5 | 2.7B | ‚ùå 45% | 0.35s | Trop verbeux (91-113 mots, hallucinations) |
| Qwen 7B Q4 | 7B | ‚úÖ 100% | 0.21s | **Parfait** mais trop lourd (hardware rejet) |
| **Qwen 1.5B Q4** | **1.5B** | **‚úÖ 87%** | **0.25s** | **PRODUCTION** (optimal ratio perf/ressources) |

---

## Configuration Optimale

### Priorit√© Endpoints

1. **LM Studio** (priorit√© 1): Qwen 2.5-1.5B local
2. **DeadBot** (priorit√© 2): FastAPI local si configur√©
3. **OpenAI GPT-4o-mini** (fallback): Si tous locaux indisponibles

### Mode ASK (questions factuelles)

**But**: R√©ponses concises et claires (‚â§250 chars r√©els)

```python
# Prompt
SYSTEM_ASK_FINAL = """Tu es serda_bot. R√©ponds de fa√ßon concise et claire.
Maximum 200 caract√®res par r√©ponse.  # Marge s√©curit√© ‚Üí r√©el ~136-220 chars

Exemples:
"python" ‚Üí "Langage populaire pour scripts et IA."
"blockchain" ‚Üí "Technologie de registre d√©centralis√© s√©curis√©."
"""

# Config LM Studio
temperature = 0.4          # D√©terministe, z√©ro hallucinations
max_tokens = 80            # ~200 chars prompt ‚Üí r√©el ‚â§250 chars
stop = ["\n\n"]           # Stop paragraphes seulement (explications compl√®tes)
repeat_penalty = 1.1       # Anti-r√©p√©tition
```

**R√©sultats**: 93.3% ‚â§250 chars, moyenne 136 chars, 0 hallucinations

### Mode CHILL (interactions sociales)

**But**: R√©ponses ultra-courtes fun/cool (1-5 mots)

```python
# Prompt
SYSTEM_CHILL_FINAL = """Tu es serda_bot, bot Twitch cool mais flemme de trop parler.
R√©ponds toujours en 1-5 mots maximum. Style d√©contract√©.

Exemples:
"Salut !" ‚Üí "Yo."
"lol" ‚Üí "Marrant."
"t'es qui toi ?" ‚Üí "Le bot du stream."
"""

# Few-shot enrichi (r√©actions + questions)
messages = [
    {"role": "user", "content": "lol"},
    {"role": "assistant", "content": "Marrant."},
    {"role": "user", "content": "t'es qui toi ?"},
    {"role": "assistant", "content": "Le bot du stream."},
    {"role": "user", "content": user_input}
]

# Config LM Studio
temperature = 0.5          # Stable et naturel
max_tokens = 20            # Ultra strict (1-5 mots)
stop = None                # Pas de stop, naturel
repeat_penalty = 1.0       # Pas de p√©nalit√©
```

**R√©sultats**: 80% ‚â§5 mots, moyenne 2.6 mots, 0.09s latence

---

## Processus d'Optimisation

### Phase 1: Tests comparatifs mod√®les
- Qwen 1.5B ‚Üí 7B ‚Üí Phi-2
- Verdict: 7B parfait mais trop lourd, 1.5B optimisable

### Phase 2: Prompt engineering
- Baseline: 55% ASK, 53% CHILL
- Limite 250 chars: 85.7% ASK, 80% CHILL
- **Limite 200 chars (marge s√©curit√©)**: 93.3% ASK ‚úÖ

### Phase 3: Few-shot enrichissement
- CHILL questions: 53% ‚Üí 80% (+27% boost)
- Ajout 2 exemples (r√©actions + questions)

### Phase 4: Temp√©ratures adaptatives
- ASK: 0.4 (d√©terministe, z√©ro hallucinations)
- CHILL: 0.5 (stable, naturel)

---

## Utilisation en Production

### Configuration LM Studio

1. **Charger le mod√®le**: `Qwen2.5-1.5B-Instruct-Q4_K_M.gguf`
2. **Endpoint**: `http://127.0.0.1:1234/v1/chat/completions`
3. **Laisser LM Studio g√©rer**: GPU layers, context, etc.

### Fichiers Config

- **Prompts**: `src/prompts/prompt_loader.py`
  - `SYSTEM_ASK_FINAL` (200 chars limit)
  - `SYSTEM_CHILL_FINAL` (1-5 mots)
  
- **API calls**: `src/utils/model_utils.py`
  - Temp√©ratures 0.4/0.5
  - Max tokens 80/20
  - Stop sequences dynamiques

### Tests de Validation

```bash
# Test ASK (93% succ√®s valid√©)
python personnal/tests/test_ask_200_chars.py

# Test CHILL (80% succ√®s valid√©)
python personnal/tests/test_chill_enriched.py

# Test production massif (85 cas)
python personnal/tests/test_production_final.py
```

---

## M√©triques Finales

### ASK Mode
- **Succ√®s**: 93.3% ‚â§250 chars (14/15 cas)
- **Moyenne**: 136 chars
- **Max observ√©**: 293 chars (reste <500 IRC ‚úÖ)
- **Latence**: 0.41s
- **Hallucinations**: 0

### CHILL Mode
- **Succ√®s**: 80% ‚â§5 mots (40/50 cas)
- **Moyenne**: 2.6 mots
- **Latence**: 0.09s

### Global
- **Succ√®s combin√©**: ~87% (largement au-dessus objectif 80%)
- **100% sous limite IRC**: Toutes r√©ponses <500 chars ‚úÖ
- **Am√©lioration vs baseline**: +38% ASK, +27% CHILL

---

## Notes Techniques

### Pourquoi limite 200 chars dans le prompt ?

**Logique**: Prompt dit "200 chars" ‚Üí mod√®le g√©n√®re ~136-220 ‚Üí toujours <250 chars cible

**Marge s√©curit√©**: Si on met "250 chars" dans prompt, certains cas d√©passent 250-363 chars r√©els

**Cas probl√©matiques r√©solus**:
- "comment fonctionne internet" : 330‚Üí128 chars ‚úÖ
- "blockchain" : 363‚Üí220 chars ‚úÖ
- "machine learning" : 271‚Üí155 chars ‚úÖ

### Temp√©ratures adaptatives

- **ASK 0.4**: D√©terministe, r√©p√©table, z√©ro hallucinations
- **CHILL 0.5**: Stable, naturel, variations acceptables pour chat

### Few-shot enrichi (CHILL)

Sans few-shot: 53% succ√®s questions ("t'es qui toi ?" ‚Üí 15 mots verbeux)
Avec few-shot: 80% succ√®s questions ("t'es qui toi ?" ‚Üí "Le bot du stream." 4 mots)

**Impact**: +27% boost questions, r√©actions d√©j√† bonnes (80% baseline)

---

## Maintenance

### Si performances d√©gradent:

1. **V√©rifier LM Studio actif**: `curl http://127.0.0.1:1234/v1/models`
2. **Tester endpoint**: `python scripts/smoke_model.py`
3. **V√©rifier prompts**: `src/prompts/prompt_loader.py` (pas modifi√©s)
4. **Re-run tests validation**: `personnal/tests/test_*.py`

### Si besoin upgrade mod√®le:

- Qwen 2.5-3B Q4 (2x plus gros, potentiel +5% succ√®s)
- Qwen 2.5-7B Q4 (si hardware upgrade, 100% succ√®s valid√©)

**Note**: Actuel 1.5B d√©j√† optimal pour contraintes hardware (87% succ√®s global)

---

## Fallback OpenAI

### GPT-4o-mini (Production)

**Choisi pour**:
- ‚úÖ 100% succ√®s ASK + CHILL (test√© 10 cas vari√©s)
- ‚úÖ 4x moins cher que GPT-3.5-turbo
  - Input: $0.15/1M tokens (vs $0.50)
  - Output: $0.60/1M tokens (vs $1.50)
- ‚úÖ Latence acceptable pour fallback (0.57-1.52s)

**Performances valid√©es**:
- ASK: 100% ‚â§250 chars, 188 chars moyenne, 1.52s
- CHILL: 100% ‚â§5 mots, 3.8 mots moyenne, 0.67s

**Config**:
```yaml
openai:
  api_key: "sk-proj-xxxxx"
bot:
  openai_model: "gpt-4o-mini"  # Fallback si LM Studio down
```

**Activation**: Automatique si LM Studio + DeadBot indisponibles
