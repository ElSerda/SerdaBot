# üéØ TODO - Optimisations Prioritaires SerdaBot

**Date de cr√©ation** : 2025-10-20  
**Auteur** : GitHub Copilot (analyse du codebase)  
**Contexte** : Suite √† l'analyse architecture compl√®te, voici les am√©liorations recommand√©es par ordre de priorit√©.

---

## üî¥ PRIORIT√â HAUTE (Production Impact)

### 1. ‚è±Ô∏è **Refactoriser les Cooldowns - Centralisation**

**Probl√®me actuel** :
- Cooldowns dispers√©s dans 3 endroits :
  - `twitch_bot.py:76` ‚Üí `self.cooldowns = {}` (cooldown global par user)
  - `model_utils.py:41` ‚Üí `_failed_endpoints = {}` (cache endpoint √©checs 2min)
  - `cache_manager.py:22` ‚Üí `_last_wiki_call` (rate limit Wikipedia 1req/s)
- Pas de cooldown sp√©cifique pour l'acc√®s LLM par user
- M√©moire RAM non optimis√©e (dict infini si beaucoup d'users)

**Solution recommand√©e** :
```python
# Cr√©er src/utils/rate_limiter.py
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Optional

class RateLimiter:
    """Gestionnaire centralis√© de tous les rate limits."""
    
    def __init__(self, max_users: int = 1000):
        # LRU cache pour limiter RAM (FIFO quand max_users atteint)
        self._user_cooldowns: OrderedDict[str, datetime] = OrderedDict()
        self._user_llm_calls: OrderedDict[str, datetime] = OrderedDict()
        self._endpoint_failures: dict[str, tuple[datetime, int]] = {}
        self._last_wiki_call: float = 0
        self._max_users = max_users
    
    def check_user_cooldown(self, user: str, cooldown_sec: int = 10) -> tuple[bool, int]:
        """V√©rifie cooldown global user (commands)."""
        # Si d√©pass√© max_users, virer le plus vieux (FIFO)
        if len(self._user_cooldowns) >= self._max_users:
            self._user_cooldowns.popitem(last=False)
        
        if user in self._user_cooldowns:
            elapsed = (datetime.now() - self._user_cooldowns[user]).total_seconds()
            if elapsed < cooldown_sec:
                return False, int(cooldown_sec - elapsed)
        
        self._user_cooldowns[user] = datetime.now()
        return True, 0
    
    def check_llm_rate_limit(self, user: str, limit_sec: int = 3) -> tuple[bool, float]:
        """Rate limit sp√©cifique LLM par user (3s entre appels)."""
        if len(self._user_llm_calls) >= self._max_users:
            self._user_llm_calls.popitem(last=False)
        
        if user in self._user_llm_calls:
            elapsed = (datetime.now() - self._user_llm_calls[user]).total_seconds()
            if elapsed < limit_sec:
                return False, limit_sec - elapsed
        
        self._user_llm_calls[user] = datetime.now()
        return True, 0.0
    
    def check_endpoint_health(self, endpoint: str) -> tuple[bool, Optional[str]]:
        """
        Backoff exponentiel pour endpoints √©chou√©s.
        1er √©chec: 30s, 2e: 1min, 3e: 2min, max: 5min
        """
        if endpoint not in self._endpoint_failures:
            return True, None
        
        fail_time, fail_count = self._endpoint_failures[endpoint]
        backoff = min(300, 30 * (2 ** fail_count))  # Max 5min
        elapsed = (datetime.now() - fail_time).total_seconds()
        
        if elapsed < backoff:
            return False, f"Endpoint en cooldown ({int(backoff - elapsed)}s, √©chec #{fail_count})"
        
        # R√©initialiser apr√®s backoff
        del self._endpoint_failures[endpoint]
        return True, None
    
    def mark_endpoint_failure(self, endpoint: str):
        """Enregistre un √©chec endpoint (incr√©mente compteur)."""
        if endpoint in self._endpoint_failures:
            _, count = self._endpoint_failures[endpoint]
            self._endpoint_failures[endpoint] = (datetime.now(), count + 1)
        else:
            self._endpoint_failures[endpoint] = (datetime.now(), 1)
    
    def mark_endpoint_success(self, endpoint: str):
        """R√©initialise compteur √©checs sur succ√®s."""
        if endpoint in self._endpoint_failures:
            del self._endpoint_failures[endpoint]
    
    def check_wiki_rate_limit(self) -> tuple[bool, float]:
        """Wikipedia: 1 requ√™te/seconde."""
        import time
        now = time.time()
        elapsed = now - self._last_wiki_call
        
        if elapsed < 1.0:
            return False, 1.0 - elapsed
        
        self._last_wiki_call = now
        return True, 0.0
    
    def get_memory_usage(self) -> dict:
        """Stats RAM pour benchmark."""
        import sys
        return {
            "user_cooldowns_count": len(self._user_cooldowns),
            "user_llm_calls_count": len(self._user_llm_calls),
            "endpoint_failures_count": len(self._endpoint_failures),
            "estimated_bytes": (
                sys.getsizeof(self._user_cooldowns) +
                sys.getsizeof(self._user_llm_calls) +
                sys.getsizeof(self._endpoint_failures)
            )
        }

# Instance globale
rate_limiter = RateLimiter(max_users=1000)
```

**Migration** :
- [ ] Cr√©er `src/utils/rate_limiter.py`
- [ ] Remplacer `self.cooldowns` dans `twitch_bot.py`
- [ ] Remplacer `_failed_endpoints` dans `model_utils.py`
- [ ] Remplacer `_last_wiki_call` dans `cache_manager.py`
- [ ] Ajouter `rate_limiter.check_llm_rate_limit(user)` dans `chill_command.py` AVANT appel LLM
- [ ] Benchmark RAM : tester avec 100/500/1000 users simul√©s

**B√©n√©fices** :
- ‚úÖ RAM ma√Ætris√©e (max 1000 users, ~50KB en m√©moire)
- ‚úÖ Rate limit LLM par user (3s) ‚Üí √©vite spam
- ‚úÖ Backoff exponentiel endpoint ‚Üí meilleure r√©silience
- ‚úÖ Code centralis√© ‚Üí 1 seul endroit √† maintenir

**Effort estim√©** : 2-3h

---

### 2. üè• **Heartbeat Model Local - Strat√©gie Recommand√©e**

**Probl√®me actuel** :
- Cache endpoint √©chec pendant 2min = d√©lai avant retry
- Pas de health check proactif
- Utilisateur attend timeout (10s) pour d√©couvrir que LM Studio est down

**Options analys√©es** :

#### ‚ùå **Option A : Heartbeat p√©riodique (d√©conseill√©)**
```python
# Ping toutes les 30s ‚Üí surcharge r√©seau inutile
async def heartbeat_loop():
    while True:
        await asyncio.sleep(30)
        await check_health(endpoint)
```
**Probl√®mes** :
- Consomme des tokens inutilement
- Surcharge r√©seau
- LM Studio peut √™tre up mais surcharg√©

#### ‚úÖ **Option B : Lazy Health Check (RECOMMAND√â)**
```python
# Dans rate_limiter.py
class RateLimiter:
    async def try_endpoint_with_health(self, endpoint: str, payload: dict) -> tuple[bool, str]:
        """
        Essaye l'endpoint avec health check rapide si r√©cemment √©chou√©.
        Si √©chec r√©cent: ping l√©ger avant vraie requ√™te.
        """
        health_ok, msg = self.check_endpoint_health(endpoint)
        
        if not health_ok:
            # Endpoint en cooldown, tenter un health check l√©ger
            try:
                async with httpx.AsyncClient(timeout=2.0) as client:
                    # Ping rapide sans payload lourd
                    response = await client.get(f"{endpoint}/health")
                    if response.status_code == 200:
                        # Endpoint est revenu, r√©initialiser
                        self.mark_endpoint_success(endpoint)
                        health_ok = True
            except Exception:
                # Toujours down, garder cooldown
                return False, msg or "Endpoint indisponible"
        
        return health_ok, ""
```

**Strat√©gie finale recommand√©e** :
```python
# Dans model_utils.py - call_model()

# 1. V√©rifier sant√© endpoint (backoff exponentiel)
health_ok, reason = rate_limiter.check_endpoint_health(api_url)
if not health_ok:
    print(f"[MODEL] ‚ö†Ô∏è {reason}, fallback direct")
    return await try_openai_fallback(...)

# 2. Essayer LM Studio
try:
    result = await try_endpoint(api_url, ...)
    if result:
        rate_limiter.mark_endpoint_success(api_url)  # R√©init compteur
        return result
    rate_limiter.mark_endpoint_failure(api_url)
except Exception:
    rate_limiter.mark_endpoint_failure(api_url)

# 3. Fallback OpenAI
return await try_openai_fallback(...)
```

**B√©n√©fices** :
- ‚úÖ Pas de heartbeat p√©riodique = 0 surcharge
- ‚úÖ Health check UNIQUEMENT quand √©chec r√©cent
- ‚úÖ Backoff exponentiel √©vite spam d'un endpoint down
- ‚úÖ Fallback imm√©diat si endpoint en cooldown

**Migration** :
- [ ] Int√©grer `check_endpoint_health()` dans `call_model()`
- [ ] Remplacer logique `_failed_endpoints` actuelle
- [ ] Ajouter logging backoff : "√âchec #3, retry dans 2min"

**Effort estim√©** : 1h

---

## üü° PRIORIT√â MOYENNE (Am√©lioration Progressive)

### 3. üìä **Structured Output - Activer Metadata en Production**

**Actuellement** : Code existe dans `prompt_loader.py` mais `extract_metadata=False` partout

**Suggestion** :
```python
# Activer UNIQUEMENT pour mode CHILL (d√©tection incertitude)
if mode == "chill":
    response = await call_model(..., extract_metadata=True)
    parsed = parse_structured_response(response)
    
    if parsed["confidence"] and parsed["confidence"] < 0.7:
        # Ajouter disclaimer
        message = f"ü§î {parsed['message']} (pas s√ªr √† 100%)"
    else:
        message = parsed["message"]
```

**B√©n√©fices** :
- Transparence sur l'incertitude du mod√®le
- D√©tection automatique des r√©ponses vagues
- Pas d'impact sur mode ASK (factuel d√©j√† valid√©)

**Migration** :
- [ ] Activer dans `chill_command.py` ligne 180
- [ ] Tester avec qwen2.5-3b (v√©rifier format JSON valide)
- [ ] Benchmark latence : +50ms acceptable ?

**Effort estim√©** : 1h

---

### 4. üíæ **Cache LLM Responses (Optionnel)**

**Id√©e** : Cache les questions fr√©quentes identiques
```python
# src/utils/llm_cache.py
from functools import lru_cache
import hashlib

class LLMResponseCache:
    def __init__(self, max_size: int = 500, ttl_hours: int = 24):
        self._cache: dict[str, tuple[str, datetime]] = {}
        self._max_size = max_size
        self._ttl = timedelta(hours=ttl_hours)
    
    def get(self, prompt: str, mode: str) -> Optional[str]:
        key = hashlib.sha256(f"{mode}:{prompt}".encode()).hexdigest()
        if key in self._cache:
            response, timestamp = self._cache[key]
            if datetime.now() - timestamp < self._ttl:
                return response
            del self._cache[key]
        return None
    
    def set(self, prompt: str, mode: str, response: str):
        if len(self._cache) >= self._max_size:
            # Virer le plus vieux
            oldest = min(self._cache.items(), key=lambda x: x[1][1])
            del self._cache[oldest[0]]
        
        key = hashlib.sha256(f"{mode}:{prompt}".encode()).hexdigest()
        self._cache[key] = (response, datetime.now())
```

**Questions √† valider** :
- [ ] Benchmark hit rate sur 1 semaine production
- [ ] RAM occup√©e (500 entr√©es ‚âà 500KB estim√©)
- [ ] Pertinence : questions r√©p√©t√©es fr√©quentes ?

**Effort estim√©** : 2h + 1 semaine observation

---

## üü¢ PRIORIT√â BASSE (Nice to Have)

### 5. üìà **Metrics Export JSON (Alternative Dashboard)**

**Contexte** : Tu as d√©j√† les logs live dans le terminal ‚úÖ

**Suggestion light** : Export JSON horaire pour analyse post-mortem
```python
# src/utils/metrics_exporter.py
class MetricsExporter:
    def __init__(self, output_dir: Path = Path("logs/metrics")):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self._hourly_stats = {
            "llm_calls": 0,
            "cache_hits": 0,
            "routing_hits": 0,
            "errors": 0,
            "avg_latency_ms": []
        }
    
    def log_llm_call(self, latency_ms: float, tokens_in: int, tokens_out: int):
        self._hourly_stats["llm_calls"] += 1
        self._hourly_stats["avg_latency_ms"].append(latency_ms)
    
    async def export_hourly(self):
        """Export toutes les heures."""
        while True:
            await asyncio.sleep(3600)
            timestamp = datetime.now().strftime("%Y%m%d_%H00")
            filepath = self.output_dir / f"metrics_{timestamp}.json"
            
            # Calculer moyennes
            stats = self._hourly_stats.copy()
            if stats["avg_latency_ms"]:
                stats["avg_latency_ms"] = sum(stats["avg_latency_ms"]) / len(stats["avg_latency_ms"])
            
            with open(filepath, "w") as f:
                json.dump(stats, f, indent=2)
            
            # Reset
            self._hourly_stats = {k: 0 if k != "avg_latency_ms" else [] for k in stats}
```

**Usage** : Analyser tendances, d√©tecter regressions apr√®s update mod√®le

**Effort estim√©** : 1h

---

## üß™ BENCHMARKS √Ä FAIRE

### Benchmark RAM Cooldowns
```bash
# Simuler 100/500/1000 users concurrents
python scripts/benchmark_rate_limiter.py --users 1000 --duration 60

# Mesurer :
# - RAM utilis√©e (via psutil)
# - Temps lookup (doit √™tre <1ms)
# - Taille dict apr√®s cleanup LRU
```

### Benchmark Cache LLM Hit Rate
```bash
# Observer 1 semaine en production
# Compter combien de fois m√™me question pos√©e
# D√©cider si pertinent (seuil: 20% hit rate minimum)
```

---

## üìù NOTES DE MIGRATION

### Ordre d'impl√©mentation recommand√© :
1. **RateLimiter** (Priority High) ‚Üí Impact imm√©diat RAM + anti-spam LLM
2. **Heartbeat strategy** (Priority High) ‚Üí Meilleure UX fallback
3. **Structured output** (Priority Medium) ‚Üí Test 1 semaine, d√©sactiver si pas concluant
4. **Cache LLM** (Priority Low) ‚Üí Observer d'abord hit rate naturel
5. **Metrics export** (Priority Low) ‚Üí Quand besoin analyse historique

### Tests de r√©gression :
- [ ] `test_full_pipeline.py` : 24/24 tests doivent passer
- [ ] `test_all_commands.py` : 17/17 tests doivent passer
- [ ] Benchmark latence : v√©rifier que rate_limiter ajoute <1ms overhead

---

## üí¨ FEEDBACK & AJUSTEMENTS

**Questions ouvertes** :
1. **RAM max acceptable** : 1000 users = ~50KB, 10k users = ~500KB. Limite √† d√©finir ?
2. **LLM rate limit** : 3s entre appels par user. Trop strict ? Trop lax ?
3. **Backoff max** : 5min actuellement. Augmenter √† 10min ?

**Si tu veux prioriser diff√©remment** : Dis-moi et j'ajuste le TODO !

---

**Derni√®re mise √† jour** : 2025-10-20  
**Statut global** : üü¢ Pr√™t pour impl√©mentation
