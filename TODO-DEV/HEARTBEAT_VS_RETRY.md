# üéØ Heartbeat vs Retry Intelligent : Analyse D√©taill√©e

> **Contexte :** Gestion fallback LM Studio (local) ‚Üí OpenAI (cloud)  
> **Probl√®me actuel :** Cache √©chec 2min hardcod√©, pas de v√©rification proactive  
> **Date :** 20 octobre 2025

---

## üìä √âtat Actuel (model_utils.py)

### Strat√©gie actuelle : Cache √©chec fixe
```python
# model_utils.py - Ligne 41-75
_failed_endpoints: dict[str, datetime] = {}
_CACHE_DURATION = timedelta(minutes=2)  # Hardcod√©

async def call_model(...):
    # Tente endpoint local
    if api_url and api_url not in _failed_endpoints:
        result = await try_endpoint(api_url, ...)
        if result:
            return result
        # √âCHEC : cache 2min
        _failed_endpoints[api_url] = now + timedelta(minutes=2)
    
    # Fallback OpenAI
    return await try_openai_fallback(...)
```

**Probl√®mes :**
- ‚ùå Timing arbitraire (pourquoi 2min ?)
- ‚ùå Pas de distinction √©chec r√©seau vs √©chec mod√®le
- ‚ùå Aucune tentative de r√©cup√©ration proactive
- ‚ùå Si local revient apr√®s 30s, attente 90s inutiles

---

## üÜö Comparaison des Options

### Option A : Heartbeat p√©riodique

**Architecture :**
```python
# model_utils.py
class ModelHealthMonitor:
    def __init__(self):
        self.endpoints = {
            "http://localhost:1234": {
                "status": "unknown",  # healthy | degraded | down | unknown
                "last_check": None,
                "consecutive_failures": 0,
                "last_successful_call": None
            }
        }
        self.health_check_task = None
    
    async def start(self):
        """Lance le heartbeat background."""
        self.health_check_task = asyncio.create_task(self._health_check_loop())
    
    async def _health_check_loop(self):
        """V√©rifie sant√© tous les 30s."""
        while True:
            for endpoint, info in self.endpoints.items():
                try:
                    # Ping l√©ger (GET /health ou HEAD /)
                    async with httpx.AsyncClient(timeout=2.0) as client:
                        response = await client.get(f"{endpoint}/health")
                        if response.status_code == 200:
                            info["status"] = "healthy"
                            info["consecutive_failures"] = 0
                        else:
                            info["status"] = "degraded"
                except:
                    info["consecutive_failures"] += 1
                    if info["consecutive_failures"] >= 3:
                        info["status"] = "down"
                
                info["last_check"] = datetime.now()
            
            await asyncio.sleep(30)  # Configurable
    
    def is_endpoint_healthy(self, endpoint: str) -> bool:
        """V√©rifie si endpoint est disponible."""
        status = self.endpoints.get(endpoint, {}).get("status", "unknown")
        return status in ["healthy", "unknown"]  # Optimiste si jamais test√©
```

**Avantages :**
- ‚úÖ **D√©tection proactive** : Sait AVANT requ√™te user si local down
- ‚úÖ **R√©cup√©ration rapide** : D√©tecte quand local revient (max 30s)
- ‚úÖ **M√©triques uptime** : Historique de sant√© endpoints
- ‚úÖ **Switch intelligent** : Peut forcer OpenAI si local instable
- ‚úÖ **Dashboard ready** : Statut temps r√©el pour monitoring

**Inconv√©nients :**
- ‚ö†Ô∏è **+1 task asyncio** : Background thread permanent
- ‚ö†Ô∏è **Appels API toutes les 30s** : N√©gligeable (HEAD request, <10ms)
- ‚ö†Ô∏è **Complexit√©** : +100 lignes de code
- ‚ö†Ô∏è **Faux positifs** : Ping OK mais mod√®le KO (rare)

**Configuration sugg√©r√©e :**
```yaml
bot:
  health_check_enabled: true
  health_check_interval: 30     # Secondes entre pings
  health_check_timeout: 2       # Timeout du ping
  health_check_failures_threshold: 3  # √âchecs avant "down"
```

**Cas d'usage id√©al :**
- üéØ Production 24/7
- üéØ SLA strict (uptime > 99%)
- üéØ Monitoring dashboard (Grafana)
- üéØ Environnement instable (LM Studio crash fr√©quents)

---

### Option B : Retry intelligent (Backoff exponentiel)

**Architecture :**
```python
# model_utils.py
class EndpointFailureTracker:
    def __init__(self):
        self.failures = {}  # {endpoint: FailureInfo}
    
    def record_failure(self, endpoint: str):
        """Enregistre un √©chec et calcule backoff."""
        if endpoint not in self.failures:
            self.failures[endpoint] = FailureInfo(count=0, first_failure=datetime.now())
        
        info = self.failures[endpoint]
        info.count += 1
        info.last_failure = datetime.now()
        
        # Backoff exponentiel : 5s ‚Üí 10s ‚Üí 20s ‚Üí 40s ‚Üí 80s ‚Üí 160s ‚Üí 300s (max)
        info.backoff_seconds = min(300, 5 * (2 ** info.count))
    
    def record_success(self, endpoint: str):
        """Reset compteur √©checs."""
        if endpoint in self.failures:
            del self.failures[endpoint]
    
    def can_retry(self, endpoint: str) -> bool:
        """V√©rifie si assez de temps √©coul√© pour retry."""
        if endpoint not in self.failures:
            return True
        
        info = self.failures[endpoint]
        elapsed = (datetime.now() - info.last_failure).total_seconds()
        return elapsed >= info.backoff_seconds

# Dans call_model()
async def call_model(...):
    if tracker.can_retry(api_url):
        result = await try_endpoint(api_url, ...)
        if result:
            tracker.record_success(api_url)  # Reset
            return result
        tracker.record_failure(api_url)  # Incr√©mente backoff
    
    # Fallback OpenAI
    return await try_openai_fallback(...)
```

**Avantages :**
- ‚úÖ **Simplicit√©** : Pas de thread background
- ‚úÖ **Adaptatif** : Backoff s'ajuste selon nombre d'√©checs
- ‚úÖ **L√©ger** : <50 lignes de code
- ‚úÖ **√âconome ressources** : Pas d'appels p√©riodiques

**Inconv√©nients :**
- ‚ö†Ô∏è **R√©actif seulement** : D√©couvre panne sur erreur user
- ‚ö†Ô∏è **D√©lai r√©cup√©ration** : Peut attendre 5min avant retry
- ‚ö†Ô∏è **Pas de m√©triques** : Pas d'historique uptime
- ‚ö†Ô∏è **Accumulation √©checs** : Plusieurs users peuvent √©chouer avant fallback

**Configuration sugg√©r√©e :**
```yaml
rate_limiting:
  llm_retry_delay: 5            # Premier retry apr√®s 5s
  llm_backoff_multiplier: 2     # Exponentiel (√ó2 √† chaque √©chec)
  llm_backoff_max: 300          # Max 5min entre retries
  llm_reset_on_success: true    # Reset compteur si succ√®s
```

**Cas d'usage id√©al :**
- üéØ Dev / Test
- üéØ Environnement stable (LM Studio fiable)
- üéØ Priorit√© simplicit√©
- üéØ Pas besoin monitoring temps r√©el

---

## üéØ Recommandation Finale

### Pour ton cas (SerdaBot) :

**Contexte :**
- Dev actuel (tests) ‚Üí Prod future (stream 24/7)
- LM Studio local (qwen2.5-3b) + OpenAI fallback
- Tests montrent : 7 users concurrent max, latence < 3s

**üî¥ Phase 1 (MAINTENANT - Dev/Test) : Option B (Retry intelligent)**

**Raisons :**
1. ‚úÖ Simple √† impl√©menter (30min)
2. ‚úÖ Pas de complexit√© additionnelle
3. ‚úÖ Suffisant pour tests/dev
4. ‚úÖ Compatible avec Option A future

**Impl√©mentation :**
```python
# src/utils/endpoint_tracker.py (nouveau fichier)
class EndpointFailureTracker:
    # ... (code ci-dessus)

# Dans model_utils.py
from src.utils.endpoint_tracker import EndpointFailureTracker
_endpoint_tracker = EndpointFailureTracker()

async def call_model(...):
    if _endpoint_tracker.can_retry(api_url):
        # Try local
        ...
    # Fallback OpenAI
```

**üü¢ Phase 2 (PROD 24/7) : Option A (Heartbeat)**

**Raisons :**
1. ‚úÖ D√©tection proactive = meilleure UX
2. ‚úÖ M√©triques uptime pour monitoring
3. ‚úÖ R√©cup√©ration rapide si local revient
4. ‚úÖ Justifi√© pour production

**Trigger migration :**
- Quand stream devient 24/7 r√©gulier
- Si LM Studio instable (>5% downtime)
- Besoin dashboard monitoring

---

## üìã Plan d'Impl√©mentation

### √âtape 1 : Retry Intelligent (cette semaine)

**Fichiers √† cr√©er :**
```
src/utils/endpoint_tracker.py  (nouveau)
```

**Fichiers √† modifier :**
```
src/utils/model_utils.py       (ligne 41-75, remplacer cache fixe)
src/config/config.yaml          (ajouter rate_limiting.llm_*)
```

**Tests :**
```bash
# Test 1 : √âchec local ‚Üí fallback OpenAI
# Test 2 : Backoff exponentiel (5s ‚Üí 10s ‚Üí 20s)
# Test 3 : Reset sur succ√®s
python scripts/test_endpoint_fallback.py
```

**Estimation :** 30-60 min

---

### √âtape 2 : Heartbeat (prod future)

**Fichiers √† cr√©er :**
```
src/utils/health_monitor.py    (nouveau)
```

**Fichiers √† modifier :**
```
src/utils/model_utils.py       (int√©grer health_monitor)
src/chat/twitch_bot.py         (start health monitor au boot)
```

**Tests :**
```bash
# Test 1 : Heartbeat d√©tecte local down en 30s
# Test 2 : Heartbeat d√©tecte local recovery
# Test 3 : Metrics uptime
python scripts/test_health_monitor.py
```

**Estimation :** 2-3 heures

---

## üîß Configuration Finale Sugg√©r√©e

```yaml
# config.yaml
bot:
  # Mod√®le local
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions
  model_timeout: 10
  
  # Fallback cloud
  model_type: "openai"
  openai_model: "gpt-4o-mini"

rate_limiting:
  # ===== PHASE 1 : Retry Intelligent (ACTUEL) =====
  llm_retry_delay: 5            # Premier retry apr√®s 5s
  llm_backoff_multiplier: 2     # Exponentiel
  llm_backoff_max: 300          # Max 5min
  
  # ===== PHASE 2 : Heartbeat (PROD FUTURE) =====
  # health_check_enabled: false  # Activer en prod
  # health_check_interval: 30    # Secondes entre pings
  # health_check_timeout: 2      # Timeout ping
  # health_check_failures_threshold: 3  # √âchecs avant "down"
```

---

## üéØ TL;DR

| Crit√®re | Retry Intelligent | Heartbeat |
|---------|-------------------|-----------|
| **Complexit√©** | üü¢ Simple (50 lignes) | üü° Moyen (150 lignes) |
| **D√©tection panne** | ‚ö†Ô∏è R√©active (sur erreur) | ‚úÖ Proactive (avant erreur) |
| **R√©cup√©ration** | ‚ö†Ô∏è Lente (jusqu'√† 5min) | ‚úÖ Rapide (max 30s) |
| **Ressources** | ‚úÖ Aucune overhead | ‚ö†Ô∏è +1 task background |
| **M√©triques** | ‚ùå Limit√©es | ‚úÖ Uptime complet |
| **Production ready** | üü° OK pour dev/test | ‚úÖ Id√©al pour prod 24/7 |

**Recommandation : Impl√©menter Retry maintenant, migrer vers Heartbeat en prod.**
