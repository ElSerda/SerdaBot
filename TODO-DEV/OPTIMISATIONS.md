# ğŸ¯ TODO - Optimisations SerdaBot

> **CrÃ©Ã© le :** 20 octobre 2025  
> **Contexte :** Suite Ã  l'analyse architecturale complÃ¨te du code  
> **PrioritÃ©s :** ğŸ”´ Haute | ğŸŸ¡ Moyenne | ğŸŸ¢ Basse

---

## ğŸ”´ PRIORITÃ‰ HAUTE

### 1. Centralisation des Cooldowns (Rate Limiting)

**ProblÃ¨me actuel :**
- Cooldowns dispersÃ©s dans 3 fichiers diffÃ©rents :
  - `twitch_bot.py` : Cooldown user global (10s par dÃ©faut)
  - `model_utils.py` : Cache d'Ã©chec endpoints (2min hardcodÃ©)
  - `cache_manager.py` : Rate limit Wikipedia (1 req/sec)
- Pas de limite par utilisateur (actuellement global)
- Configuration mixte (hardcodÃ© + YAML)

**Solution proposÃ©e :**
```
src/utils/rate_limiter.py
â”œâ”€â”€ RateLimiter (classe centralisÃ©e)
â”‚   â”œâ”€â”€ user_cooldowns: Dict[str, datetime]  # Cooldown par user
â”‚   â”œâ”€â”€ endpoint_failures: Dict[str, FailureInfo]  # Cache Ã©checs avec backoff
â”‚   â”œâ”€â”€ api_rate_limits: Dict[str, RateLimit]  # Wikipedia, IGDB, etc.
â”‚   â””â”€â”€ cleanup_expired()  # Nettoyage automatique
```

**ParamÃ¨tres Ã  ajouter dans `config.yaml` :**
```yaml
rate_limiting:
  # User rate limiting (anti-spam)
  user_cooldown: 10              # Secondes entre messages (par user)
  max_requests_per_user_hour: 20 # Max messages par user/heure
  
  # LLM endpoint management
  llm_retry_delay: 5             # DÃ©lai avant retry endpoint (secondes)
  llm_backoff_max: 300           # Backoff max (5min)
  llm_backoff_multiplier: 2      # Exponentiel (5s â†’ 10s â†’ 20s â†’ ...)
  
  # External APIs
  wikipedia_rate_limit: 1.0      # 1 req/sec
  igdb_rate_limit: 4.0           # 4 req/sec (limite Twitch API)
```

**ImplÃ©mentation :**
1. CrÃ©er `src/utils/rate_limiter.py` avec classe `RateLimiter`
2. Migrer `self.cooldowns` de `twitch_bot.py` vers RateLimiter
3. Migrer `_failed_endpoints` de `model_utils.py` vers RateLimiter
4. Migrer `_last_wiki_call` de `cache_manager.py` vers RateLimiter
5. Ajouter backoff exponentiel pour endpoint failures

**BÃ©nÃ©fices :**
- âœ… Code maintenu Ã  un seul endroit
- âœ… Configuration unifiÃ©e dans YAML
- âœ… Backoff intelligent (Ã©vite spam retry)
- âœ… Meilleure gestion RAM (cleanup automatique)

**Estimation RAM :** Voir benchmark `TODO-DEV/BENCHMARK_RAM_COOLDOWNS.md`

---

### 2. Heartbeat Model Local vs Fallback Cloud

**Contexte :**
- Actuellement : retry immÃ©diat sur Ã©chec â†’ cache 2min â†’ fallback OpenAI
- ProblÃ¨me : Pas de vÃ©rification proactive de santÃ© du modÃ¨le local

**Options proposÃ©es :**

#### **Option A : Heartbeat pÃ©riodique (RECOMMANDÃ‰ pour prod 24/7)**
```python
# Dans model_utils.py
async def health_check_loop():
    """VÃ©rifie la santÃ© du endpoint local toutes les 30s"""
    while True:
        try:
            response = await client.get(f"{LM_STUDIO_URL}/health", timeout=2.0)
            if response.status_code == 200:
                mark_endpoint_healthy(LM_STUDIO_URL)
            else:
                mark_endpoint_degraded(LM_STUDIO_URL)
        except:
            mark_endpoint_down(LM_STUDIO_URL)
        await asyncio.sleep(30)
```

**Avantages :**
- âœ… DÃ©tection proactive des pannes (avant requÃªte user)
- âœ… Switch automatique vers OpenAI si local down
- âœ… Restauration automatique quand local revient
- âœ… MÃ©triques de uptime prÃ©cises

**InconvÃ©nients :**
- âš ï¸ +1 thread background
- âš ï¸ Appels API toutes les 30s (nÃ©gligeable)

**Config suggÃ©rÃ©e :**
```yaml
bot:
  health_check_enabled: true
  health_check_interval: 30  # Secondes
  health_check_timeout: 2    # Timeout du ping (2s)
```

#### **Option B : Retry intelligent (ACTUEL amÃ©liorÃ©)**
```python
# Backoff exponentiel au lieu de cache fixe 2min
failure_count = get_failure_count(endpoint)
backoff = min(300, 5 * (2 ** failure_count))  # 5s â†’ 10s â†’ 20s â†’ 40s â†’ 80s â†’ 160s â†’ 300s (max)
```

**Avantages :**
- âœ… Pas de thread background
- âœ… Simple Ã  implÃ©menter
- âœ… Ã‰conomise ressources

**InconvÃ©nients :**
- âš ï¸ DÃ©tection rÃ©active (dÃ©couverte panne sur erreur user)
- âš ï¸ Peut accumuler plusieurs Ã©checs users avant fallback

**Recommandation finale :**
- **Dev/Test :** Option B (retry intelligent) - simplicitÃ©
- **Prod 24/7 :** Option A (heartbeat) - fiabilitÃ©

---

### 3. Structured Output (Metadata) en Production

**Ã‰tat actuel :**
- Code existe dans `prompt_loader.py` : `get_response_format(extract_metadata=True)`
- Format JSON : `{"m": "message", "t": "tone", "c": confidence}`
- **Jamais activÃ© en production** (extract_metadata=False par dÃ©faut)

**Proposition : Activer en mode CHILL uniquement**

**Pourquoi ?**
- ASK : RÃ©ponses factuelles (confidence implicite)
- CHILL : RÃ©ponses conversationnelles (besoin de tonalitÃ©)

**ImplÃ©mentation :**
```python
# Dans chill_command.py
response = await call_model(..., extract_metadata=True)
parsed = parse_structured_response(response)

# Ajuster message selon confidence
if parsed["confidence"] and parsed["confidence"] < 0.6:
    msg = f"ğŸ¤” {parsed['message']} (pas sÃ»r Ã  100%)"
else:
    msg = parsed["message"]

# Adapter Ã©moji selon tone
emoji_map = {
    "complice": "ğŸ˜",
    "taquin": "ğŸ˜œ",
    "hype": "ğŸ”¥",
    "sarcastique": "ğŸ˜",
    "neutre": ""
}
final_msg = f"{emoji_map.get(parsed['tone'], '')} {msg}"
```

**BÃ©nÃ©fices :**
- âœ… Transparence sur incertitude du modÃ¨le
- âœ… Messages plus vivants (Ã©mojis contextuels)
- âœ… DÃ©tection automatique rÃ©ponses vagues

**CoÃ»t :**
- âš ï¸ +10-15 tokens par requÃªte (JSON overhead)
- âš ï¸ TempÃ©rature 0.4 requise (vs 0.5 actuel) pour JSON stable

**Config suggÃ©rÃ©e :**
```yaml
bot:
  extract_metadata_chill: true   # Activer metadata en mode CHILL
  metadata_confidence_threshold: 0.6  # Seuil disclaimer "pas sÃ»r"
```

---

## ğŸŸ¡ PRIORITÃ‰ MOYENNE

### 4. Cache LLM Responses (Optionnel)

**Contexte :**
- Questions frÃ©quentes : "c'est quoi Python ?", "salut", "gg"
- Actuellement : appel LLM Ã  chaque fois (~300-800ms)

**Proposition : Cache mÃ©moire avec TTL**
```python
# src/utils/llm_cache.py
from functools import lru_cache
import hashlib

_llm_cache = {}  # {hash(prompt): (response, timestamp)}
_CACHE_TTL = 3600  # 1 heure
_MAX_ENTRIES = 1000

def get_cached_response(prompt: str, mode: str) -> Optional[str]:
    key = hashlib.md5(f"{mode}:{prompt}".encode()).hexdigest()
    if key in _llm_cache:
        response, timestamp = _llm_cache[key]
        if time.time() - timestamp < _CACHE_TTL:
            return response
    return None
```

**BÃ©nÃ©fices :**
- âœ… Latence <1ms pour rÃ©ponses cachÃ©es
- âœ… Ã‰conomie tokens OpenAI (fallback)
- âœ… Meilleure expÃ©rience user (rÃ©ponses instantanÃ©es)

**Estimation RAM :**
- 1000 entrÃ©es Ã— ~200 chars/entrÃ©e = ~200KB (nÃ©gligeable)

**Config suggÃ©rÃ©e :**
```yaml
bot:
  llm_cache_enabled: true
  llm_cache_ttl: 3600      # 1 heure
  llm_cache_max_entries: 1000
```

---

### 5. MÃ©triques Export JSON (Optionnel)

**Contexte :**
- Logs actuels : terminaux en temps rÃ©el (excellent pour debug)
- Manque : agrÃ©gation historique, analyse tendances

**Proposition : Export JSON pÃ©riodique**
```python
# src/utils/metrics_exporter.py
class MetricsExporter:
    def export_hourly(self):
        metrics = {
            "timestamp": time.time(),
            "llm": {
                "calls_total": self.llm_calls,
                "avg_latency_ms": self.avg_latency,
                "tokens_per_second": self.avg_throughput,
                "error_rate": self.error_count / self.llm_calls
            },
            "routing": {
                "to_igdb": self.routing_igdb,
                "to_llm": self.routing_llm
            },
            "cache": {
                "wiki_hits": self.cache_hits,
                "wiki_misses": self.cache_misses,
                "hit_rate": self.cache_hits / (self.cache_hits + self.cache_misses)
            }
        }
        Path("logs/metrics").mkdir(exist_ok=True)
        with open(f"logs/metrics/{datetime.now():%Y%m%d_%H}.json", "w") as f:
            json.dump(metrics, f, indent=2)
```

**BÃ©nÃ©fices :**
- âœ… Analyse post-mortem (rÃ©gressions, bottlenecks)
- âœ… Graphiques Grafana/dashboards (optionnel)
- âœ… Benchmark A/B testing (comparer modÃ¨les)

**InconvÃ©nients :**
- âš ï¸ Redondance avec logs terminaux actuels
- âš ï¸ ComplexitÃ© additionnelle

**Recommandation :** ImplÃ©menter SEULEMENT si besoin d'analyse historique (pas urgent)

---

## ğŸŸ¢ PRIORITÃ‰ BASSE

### 6. Documentation Architecture

**Ã‰tat actuel :**
- Code trÃ¨s bien documentÃ© (docstrings, type hints)
- Manque : diagrammes de sÃ©quence, ADR (Architecture Decision Records)

**Propositions :**
```
docs/
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ PIPELINE_FLOW.md      # Diagramme sÃ©quence messageâ†’rÃ©ponse
â”‚   â”œâ”€â”€ FALLBACK_STRATEGY.md  # LM Studio â†’ OpenAI fallback
â”‚   â””â”€â”€ ROUTING_LOGIC.md      # Proactive routing dÃ©cisions
â”œâ”€â”€ adr/
â”‚   â”œâ”€â”€ 001-proactive-routing.md
â”‚   â”œâ”€â”€ 002-post-filtering.md
â”‚   â””â”€â”€ 003-wikipedia-cache.md
```

**BÃ©nÃ©fice :** Onboarding nouveaux contributeurs

---

## ğŸ“Š BENCHMARKS Ã€ FAIRE

### Benchmark RAM : User Cooldowns

**Fichier :** `TODO-DEV/BENCHMARK_RAM_COOLDOWNS.md`

**Objectif :** Mesurer impact RAM des cooldowns par user

**ScÃ©narios :**
1. **100 users actifs :** Pic stream moyen
2. **1000 users actifs :** Gros raid
3. **10,000 users actifs :** Stream viral

**MÃ©triques :**
- MÃ©moire utilisÃ©e (MB)
- Temps cleanup (ms)
- Performance lookup (Âµs)

**ImplÃ©mentation :** Script Python avec `memory_profiler`

---

## ğŸ¯ RÃ‰SUMÃ‰ PRIORISATION

| TÃ¢che                        | PrioritÃ© | Effort | Impact | Deadline suggÃ©rÃ© |
|------------------------------|----------|--------|--------|------------------|
| Centralisation cooldowns     | ğŸ”´ Haute | 4h     | â­â­â­â­â­ | Cette semaine    |
| Heartbeat model local        | ğŸ”´ Haute | 2h     | â­â­â­â­   | Cette semaine    |
| Structured output metadata   | ğŸ”´ Haute | 2h     | â­â­â­â­   | Ce mois          |
| Cache LLM responses          | ğŸŸ¡ Moyen | 3h     | â­â­â­     | Optionnel        |
| MÃ©triques export JSON        | ğŸŸ¡ Moyen | 3h     | â­â­       | Optionnel        |
| Documentation architecture   | ğŸŸ¢ Basse | 4h     | â­â­       | Quand temps      |
| Benchmark RAM cooldowns      | ğŸ”´ Haute | 1h     | â­â­â­â­   | Avant implÃ©mentation |

---

## ğŸ“ NOTES

- **Score actuel : 9.2/10** - Code dÃ©jÃ  excellent, optimisations = polish
- **Production-ready :** OUI (tests 100%, error handling complet)
- **Architecture unique :** Proactive routing + post-filter = top 5% industrie
- **KISS philosophy :** PrÃ©server simplicitÃ© dans implÃ©mentations

---

**Prochaine Ã©tape suggÃ©rÃ©e :**
1. Lancer benchmark RAM cooldowns
2. ImplÃ©menter `RateLimiter` centralisÃ©
3. Choisir stratÃ©gie heartbeat vs retry intelligent
