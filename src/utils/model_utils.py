"""Utilities for communicating with the configured LLM endpoints.

This module implements a small priority/fallback strategy:
- Primary: local LM Studio endpoint (configured in config['bot']['model_endpoint'])
- Fallback: OpenAI Async client if an API key is present

The implementation intentionally keeps behavior simple and robust:
- No DeadBot or other secondary local endpoints are contacted.
- Endpoints that fail are cached for a short duration to avoid retry storms.
"""

from __future__ import annotations

import sys
import os
import time
from datetime import datetime, timedelta
from typing import Optional

import httpx

# Ajouter le r√©pertoire parent au path pour les imports
if os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')) not in sys.path:
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from prompts.prompt_loader import load_system_prompt

# Token and temperature defaults (empirical)
MAX_TOKENS_ASK = 120
MAX_TOKENS_CHILL = 60    # Optimal pour r√©ponses courtes et percutantes (2 phrases max)
TEMP_ASK = 0.4
TEMP_CHILL = 0.6         # R√©duit de 0.7‚Üí0.6 pour moins d'al√©atoire


def estimate_tokens(text: str) -> int:
    """Estimate token count using simple heuristic (4 chars ‚âà 1 token)."""
    if not text:
        return 0
    return max(1, len(text) // 4)


# Simple in-memory cache of failed endpoints to avoid immediate retries
_failed_endpoints: dict[str, datetime] = {}
_CACHE_DURATION = timedelta(minutes=2)


async def call_model(
    prompt: str,
    config: dict,
    user: Optional[str] = None,
    timeout: Optional[int] = None,
    mode: str = "chill",
) -> Optional[str]:
    """Call the preferred model endpoint, falling back to OpenAI if needed.

    Returns:
        str: LLM response content (success)
        None: All LLMs unavailable ‚Üí caller should use fallback responses
    
    Cascade behavior:
        1. Try LM Studio (local) ‚Üí if success, return response
        2. Try OpenAI (cloud) ‚Üí if success, return response  
        3. All failed ‚Üí return None (fallback to fun responses)
    
    Note: None means "no LLM available", empty string "" means "LLM responded but empty".
    """

    effective_timeout = timeout if timeout is not None else config.get("bot", {}).get("model_timeout", 10)
    print(f"[MODEL] ‚è±Ô∏è Timeout configur√©: {effective_timeout}s")

    # Expire old failed endpoints
    now = datetime.now()
    expired = [k for k, v in _failed_endpoints.items() if now - v > _CACHE_DURATION]
    for k in expired:
        del _failed_endpoints[k]
        print(f"[MODEL] üîÑ R√©activation de {k}")

    api_url = config.get("bot", {}).get("model_endpoint") or config.get("bot", {}).get("api_url")

    # Try LM Studio-like endpoint if configured and not recently marked failed
    if api_url and api_url not in _failed_endpoints:
        print("[MODEL] üîó Tentative LM Studio...")
        result = await try_endpoint(api_url, prompt, user, effective_timeout, endpoint_type="lm_studio", mode=mode, config=config)
        if result:
            return result
        _failed_endpoints[api_url] = now
        print("[MODEL] ‚ö†Ô∏è Endpoint local indisponible, passer au fallback")

    print("[MODEL] üåê Utilisation du fallback OpenAI (si configur√©)")
    return await try_openai_fallback(prompt, config, user, mode)


async def try_endpoint(
    api_url: str,
    prompt: str,
    user: Optional[str],
    timeout: int,
    endpoint_type: str = "lm_studio",
    mode: str = "chill",
    config: Optional[dict] = None,
) -> str:
    """Attempt to call a single HTTP endpoint and return the text result.

    The function is tolerant: any networking or parsing error returns an empty string.
    """

    try:
        input_chars = len(prompt)
        input_tokens = estimate_tokens(prompt)
        print(f"[METRICS] üì• INPUT: {input_chars} chars, ~{input_tokens} tokens")
        print(f"[DEBUG] üìÑ USER Prompt: {prompt}")

        system_prompt = load_system_prompt(mode=mode)
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]

        max_tokens = MAX_TOKENS_ASK if mode == "ask" else MAX_TOKENS_CHILL
        temperature = TEMP_ASK if mode == "ask" else TEMP_CHILL
        repeat_penalty = 1.05  # L√©g√®re p√©nalit√© pour √©viter r√©p√©titions bizarres

        model_name = config.get("bot", {}).get("model_name", "local-model") if config else "local-model"

        payload = {
            "model": model_name,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "repeat_penalty": repeat_penalty,
            "top_p": 0.9,  # Nucleus sampling pour coh√©rence
        }

        start_time = time.time()
        async with httpx.AsyncClient() as client:
            response = await client.post(api_url, json=payload, timeout=timeout)
            if response.status_code != 200:
                print(f"[MODEL] ‚ùå {endpoint_type.upper()} error: {response.status_code}")
                return ""

            duration = time.time() - start_time
            data = response.json()

            # Compatible avec format OpenAI-like ou LM Studio
            result = ""
            if isinstance(data, dict):
                if "choices" in data and data["choices"]:
                    # OpenAI-like
                    choice = data["choices"][0]
                    if isinstance(choice, dict) and choice.get("message"):
                        result = choice["message"].get("content", "")
                else:
                    # LM Studio sometimes returns {'response': '...'}
                    result = data.get("response", "") or data.get("text", "")

            result = (result or "").strip()
            if not result:
                print("[MODEL] ‚ö†Ô∏è R√©ponse vide re√ßue de l'endpoint")
                return ""

            output_chars = len(result)
            output_tokens = estimate_tokens(result)
            tokens_per_sec = output_tokens / duration if duration > 0 else 0

            print(f"[METRICS] üì§ OUTPUT: {output_chars} chars, ~{output_tokens} tokens")
            print(f"[METRICS] ‚ö° Dur√©e: {duration:.2f}s, {tokens_per_sec:.1f} tok/s")
            print(f"[MODEL] ‚úÖ {endpoint_type.upper()} r√©ponse compl√®te")
            print(f"[DEBUG] üí¨ OUTPUT: {result}")
            return result

    except Exception as e:  # network/parsing errors
        print(f"[MODEL] ‚ùå {endpoint_type.upper()} failed: {e}")
        return ""


async def try_openai_fallback(prompt: str, config: dict, user: Optional[str], mode: str = "chill") -> Optional[str]:
    """Fallback to OpenAI Async client.
    
    Returns:
        str: OpenAI response (success)
        None: No API key or OpenAI failed
    """

    try:
        # Dynamic import to avoid hard dependency at import time
        from openai import AsyncOpenAI  # type: ignore

        api_key = config.get("openai", {}).get("api_key")
        if not api_key or not api_key.startswith("sk-"):
            print("[MODEL] ‚ö†Ô∏è Pas de cl√© OpenAI configur√©e")
            return None

        input_chars = len(prompt)
        input_tokens = estimate_tokens(prompt)
        print(f"[METRICS] üì• INPUT: {input_chars} chars, ~{input_tokens} tokens")

        client = AsyncOpenAI(api_key=api_key)
        model = config.get("bot", {}).get("openai_model", "gpt-4o-mini")

        system_prompt = load_system_prompt(mode=mode)
        start_time = time.time()

        max_tokens = MAX_TOKENS_ASK if mode == "ask" else MAX_TOKENS_CHILL
        temperature = TEMP_ASK if mode == "ask" else TEMP_CHILL

        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
        )

        duration = time.time() - start_time
        result = ""
        try:
            result = getattr(response.choices[0].message, "content", "") or str(response)
        except Exception:
            result = str(response)

        result = (result or "").strip()
        if not result:
            print("[MODEL] ‚ö†Ô∏è R√©ponse vide d'OpenAI")
            return None

        output_chars = len(result)
        output_tokens = estimate_tokens(result)
        tokens_per_sec = output_tokens / duration if duration > 0 else 0

        print(f"[METRICS] üì§ OUTPUT: {output_chars} chars, ~{output_tokens} tokens")
        print(f"[METRICS] ‚ö° Dur√©e: {duration:.2f}s, {tokens_per_sec:.1f} tok/s")
        print("[MODEL] ‚úÖ OPENAI fallback utilis√©")
        return result

    except Exception as e:
        print(f"[MODEL] ‚ùå OpenAI fallback failed: {e}")
        return None
