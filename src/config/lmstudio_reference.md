# ğŸ›ï¸ LM Studio - Configuration de RÃ©fÃ©rence

Ce fichier documente les **paramÃ¨tres optimaux** pour LM Studio avec SerdaBot.

---

## ğŸ¯ TL;DR : Un seul preset suffit !

**LM Studio ne permet qu'un seul profil de paramÃ¨tres Ã  la fois.**  
â†’ Configure avec le preset **"Standard Twitch"** ci-dessous.  
â†’ Les variations de comportement (chill/hype) sont gÃ©rÃ©es **dans les prompts systÃ¨me**, pas dans LM Studio.

---

## âš™ï¸ Preset "Standard Twitch" (UNIQUE)

**Ã€ configurer dans LM Studio :**

```yaml
Temperature:         0.7
Top-K:               40
Top-P:               0.9
Min-P:               0.05
Repeat Penalty:      1.10
Max Tokens:          50
Limit response:      ON
Stop Sequences:      ["\n", "User:", "Assistant:", "@"]
```

**Ce preset fonctionne pour toutes les commandes** (`!chill`, `!ask`, `!game`) car :
- Le **system prompt** est chargÃ© depuis `prompt_system.txt` (universel)
- Les **user prompts** sont gÃ©nÃ©rÃ©s dynamiquement par `make_prompt()` dans `prompt_loader.py`
- Easter Egg pour El_Serda gÃ©rÃ© automatiquement dans `make_prompt()`
- Pas besoin de changer la config LM Studio selon la commande

---

## ğŸ“ Architecture des Prompts (Nouveau SystÃ¨me)

### **Principe : 1 SYSTEM + 1 USER dynamique**

```python
# SYSTEM prompt (universel, dans prompt_system.txt)
"Tu es serda_bot, un bot Twitch francophone, fun et complice.
RÃˆGLES GÃ‰NÃ‰RALES :
- Toujours UNE SEULE phrase, naturelle, 20 Ã  30 mots MAX ou 150 caractÃ¨res MAX.
- ZÃ©ro commande /me, zÃ©ro hashtag. 0 Ã  2 Ã©mojis max.
- Style Twitch : direct, joueur, jamais agressif.
- Si tu reconnais le pseudo 'El_Serda' â†’ mode roast gentil ğŸ˜ˆ"

# USER prompt (dynamique, gÃ©nÃ©rÃ© par make_prompt())
"Contexte: Jeu=Valorant, Titre=Rank up chill.
Le message vient de ton crÃ©ateur (el_serda). Active ton mode 'roast'.
Viewer dit: Â«toujours aussi gentil serda_botÂ». RÃ©ponds sur un ton complice et fun."
```

### **Avantages du nouveau systÃ¨me :**
- âœ… **Un seul SYSTEM prompt** â†’ cohÃ©rence garantie
- âœ… **Easter Egg automatique** â†’ dÃ©tection d'El_Serda dans `make_prompt()`
- âœ… **Contexte dynamique** â†’ game/title injectÃ©s automatiquement
- âœ… **Suppression des anciens fichiers** â†’ `prompt_chill_elserda.txt`, `prompt_ask_fr.txt` obsolÃ¨tes
- âœ… **Code plus simple** â†’ pas de gestion manuelle de fichiers dans les commandes

### **Modes disponibles dans make_prompt() :**
- `ask` : Questions du chat ("Quelle est la capitale de la France ?")
- `chill` : Interactions casual ("Salut le bot !")
- `trad` : Traduction automatique (FR â†” EN)
- `reactor` : RÃ©actions aux spam du chat ("LUL LUL LUL")

---

## ğŸ¯ Explication des paramÃ¨tres

### **Temperature (0.7)**
- ContrÃ´le la crÃ©ativitÃ© du modÃ¨le
- `0.6` = Plus safe, moins d'impro (Chill/Hello)
- `0.7` = **Ã‰quilibrÃ©** (recommandÃ© par dÃ©faut)
- `0.8` = Plus fun, plus piquant (Hype mode)

### **Top-K (40)**
- Limite aux 40 meilleurs tokens
- Coupe les choix improbables
- Montez Ã  60 pour mode Hype

### **Top-P (0.9)**
- SÃ©lectionne les 90% meilleurs tokens (cumul de probabilitÃ©)
- ComplÃ©mentaire Ã  Top-K
- `0.85` = Plus strict, `0.92` = Plus variÃ©

### **Min-P (0.05)**
- Coupe les "queues bizarres" de probabilitÃ©
- Laisse TOUJOURS activÃ© !
- Ã‰vite les tokens vraiment improbables

### **Repeat Penalty (1.10)**
- PÃ©nalise la rÃ©pÃ©tition de tokens
- `1.10` = Ã‰quilibrÃ©
- `1.15` = Si le bot radote trop
- `1.05` = Mode Hype (peut rÃ©pÃ©ter pour emphase)

### **Max Tokens (50)**
- **50 tokens â‰ˆ 12-25 mots** (parfait pour Twitch one-liner)
- RÃ©duit la latence sur GTX 780
- Plus fiable que "Limit response length" seul

### **Stop Sequences**
- `\n` : Coupe Ã  la premiÃ¨re ligne (garantit 1 phrase)
- `User:` / `Assistant:` : Ã‰vite que le modÃ¨le simule un dialogue
- `@` : Ã‰vite les mentions intempestives

---

## ğŸš€ Performance attendue

### **Machine moderne** (GTX 1660+ / RTX)
- Vitesse : **50-100 tok/s**
- Latence : **~1-2 secondes**
- QualitÃ© : Excellente

### **Machine ancienne** (GTX 780 / CPU)
- Vitesse : **3-10 tok/s**
- Latence : **~3-5 secondes**
- QualitÃ© : TrÃ¨s bonne (sous timeout 10s)

---

## ğŸ“ System Prompt utilisÃ© par le bot

```
Bot Twitch FR, UNE phrase (12-25 mots), ton fun/complice,
pas de /me, 0-2 Ã©mojis, pas d'auto-flatterie.
```

**Pourquoi ce format strict ?**
- âœ… Force des rÃ©ponses courtes (Twitch = latence critique)
- âœ… Ã‰vite les pavÃ©s et les listes
- âœ… Bloque l'auto-congratulation ("ma liste de qualitÃ©s incroyables...")
- âœ… Ton lÃ©ger et fun, pas corporate

---

## ğŸ”§ Configuration dans LM Studio

1. **Lancer LM Studio**
2. Aller dans **Developer** â†’ **Server**
3. DÃ©marrer le serveur sur `http://localhost:1234/v1`
4. Charger un modÃ¨le (ex: Qwen 2.5 1.5B Instruct)
5. Appliquer les paramÃ¨tres ci-dessus dans l'interface
6. Le bot se connectera automatiquement !

---

## ğŸ® ModÃ¨les recommandÃ©s

### **Petit & Rapide** (1-3B params)
- âœ… **Qwen 2.5 1.5B Instruct** (recommandÃ©)
- âœ… Phi-3 Mini (3.8B)
- Bon pour : Machines lentes, latence minimale

### **Moyen** (7-8B params)
- âœ… Qwen 2.5 7B Instruct
- âœ… Llama 3.2 8B
- Bon pour : RTX 3060+, meilleure qualitÃ©

### **Gros** (14B+ params)
- Qwen 2.5 14B Instruct
- Llama 3.1 15B
- Bon pour : RTX 4070+, qualitÃ© maximale

---

## ğŸ’¡ Tips

- **CPU uniquement** : Utilisez un modÃ¨le â‰¤3B (Qwen 2.5 1.5B)
- **GPU <8GB VRAM** : ModÃ¨le â‰¤7B
- **GPU â‰¥12GB VRAM** : ModÃ¨le 14B+ possible
- **Twitch = latence critique** : PrivilÃ©giez la vitesse sur la taille

---

## ğŸ†˜ DÃ©pannage

### Le bot timeout (>10s)
- âš ï¸ ModÃ¨le trop gros pour ta machine
- Solution : Passer Ã  un modÃ¨le plus petit (1.5B)

### RÃ©ponses trop courtes/vides
- âš ï¸ `max_tokens` trop bas
- Solution : Augmenter Ã  100 (mais latence +)

### RÃ©ponses gÃ©nÃ©riques ("ma liste de qualitÃ©s...")
- âœ… Le garde-fou lexical filtre Ã§a automatiquement !
- Le bot rÃ©essaiera avec un meilleur prompt

---

ğŸ“… **DerniÃ¨re mise Ã  jour** : Optimisations Twitch (Oct 2025)
