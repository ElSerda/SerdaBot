# ===== SERDABOT CONFIGURATION EXAMPLE =====
# 
# üìå INSTRUCTIONS :
# 1. Cr√©er le dossier local : mkdir -p ../SerdaBot-local/config
# 2. Copier ce fichier : cp src/config/config.example.yaml ../SerdaBot-local/config/config.yaml
# 3. Remplir les vraies valeurs (tokens, cl√©s API, etc.)
# 4. Lancer le bot : ./start_bot.sh (d√©tecte automatiquement la config locale)
#
# ‚ö†Ô∏è  NE JAMAIS COMMITER config.yaml avec de vraies cl√©s !

bot:
  language: "fr"                            # Langue par d√©faut ("en" ou "fr")
  name: "your_bot_name"                     # Nom d'utilisateur Twitch du bot (login, pas display name)
  channel: "your_channel_name"              # Nom du channel Twitch (sans @)
  
  # === LLM Configuration (OPTIONAL) ===
  # Le bot fonctionne parfaitement avec ou sans LLM local
  # Si LM Studio n'est pas disponible ‚Üí mode fallback automatique
  llm:
    enabled: auto                           # auto (d√©tection automatique) | true (force) | false (d√©sactive)
    fallback_mode: fun                      # fun (r√©pliques humoristiques) | silent (neutre) | minimal (emoji)
  
  model_path: "src/model"                   # Chemin du dossier du mod√®le local
  model_file: "your-model.gguf"             # Nom du fichier GGUF du mod√®le
  model_type: "openai"                      # Type de mod√®le ("mistral", "llama", "openai", etc.)
  openai_model: "gpt-3.5-turbo"             # Mod√®le OpenAI √† utiliser
  use_gpu: true                             # Activer l'acc√©l√©ration GPU (true/false)
  gpu_layers: 50                            # Nombre de layers GPU
  cooldown: 10                              # Cooldown entre r√©ponses (en secondes)
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions  # Endpoint LM Studio
  api_url: http://127.0.0.1:1234/v1/chat/completions         # API LM Studio
  model_name: "qwen2.5-3b-instruct"         # Nom du mod√®le (Qwen2.5-3B-Instruct-Q4_K_M recommand√©)
  model_timeout: 10                         # Timeout requ√™tes mod√®le (secondes)
  max_tokens_ask: 120                       # Max tokens mode ASK (r√©ponses d√©taill√©es)
  max_tokens_chill: 60                      # Max tokens mode CHILL (conversations)
  temperature_ask: 0.4                      # Temperature ASK (factuel)
  temperature_chill: 0.7                    # Temperature CHILL (cr√©atif)
  log_ia: true                              # Activer logs IA (debug)
  debug: true                               # Verbose output (debug)
  translation: libretranslate               # Service de traduction ("model" ou "libretranslate")
  reset_cache_on_boot: false                # Reset cache au d√©marrage (default: false)
  kofi_url: "https://ko-fi.com/your_username"       # URL Ko-fi pour donations
  donation_message: "‚òï Merci pour le support ! Tu peux soutenir [YourName] ici : {kofi_url} üíú"
  connect_message: "Coucou ‚òï"                       # Message de connexion Twitch (vide = d√©sactiv√©)

  enabled_commands:
    - game
    - chill
    - ask
    - trad

openai:
  api_key: "sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"  # Cl√© API OpenAI (depuis platform.openai.com)

twitch:
  token: "oauth:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"   # Token OAuth Twitch (depuis twitchtokengenerator.com)
  client_id: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"     # Client ID Twitch (dev.twitch.tv)
  client_secret: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" # Client Secret Twitch
  bot_id: "123456789"                                # User ID du bot Twitch

igdb:
  client_id: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"     # Client ID IGDB (dev.twitch.tv)
  client_secret: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" # Client Secret IGDB

rawg:
  api_key: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"       # Cl√© API RAWG (rawg.io/apidocs)

# ===== Rate Limiting & Performance =====
rate_limiting:
  # User cooldowns (anti-spam)
  user_cooldown: 10                    # Secondes entre messages par user
  max_requests_per_user_hour: 20       # Limite anti-spam par user/heure
  max_concurrent_users: 4              # Max users simultan√©s
  
  # Cleanup (optimisation m√©moire)
  cleanup_interval: 600                # Nettoyage toutes les 10min
  max_idle_time: 3600                  # Supprimer users inactifs > 1h
  
  # LLM endpoint failure handling (retry intelligent)
  llm_retry_delay: 5                   # Premier retry apr√®s 5s d'√©chec
  llm_backoff_multiplier: 2            # Backoff exponentiel (5s ‚Üí 10s ‚Üí 20s)
  llm_backoff_max: 300                 # Backoff max 5min entre retries
  llm_reset_on_success: true           # Reset compteur √©checs si succ√®s
  
  # Health check (heartbeat - optionnel en prod 24/7)
  health_check_enabled: false          # Activer pour monitoring proactif
  health_check_interval: 30            # Secondes entre pings sant√© endpoint
  health_check_timeout: 2              # Timeout ping sant√©
  health_check_failures_threshold: 3   # √âchecs cons√©cutifs avant marquer "down"
  
  # External APIs rate limits
  wikipedia_rate_limit: 1.0            # 1 requ√™te/sec (Wikipedia)
  igdb_rate_limit: 4.0                 # 4 requ√™tes/sec (Twitch API)
