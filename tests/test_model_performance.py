"""
Test de performance et validation du mod√®le V3
==============================================
V√©rifie que le mod√®le g√©n√®re des r√©ponses coh√©rentes, concises et compl√®tes
Inclut un warmup pour des m√©triques stables
"""
import time

import pytest

from src.config.config import load_config
from src.prompts.prompt_loader import make_prompt
from src.utils.model_utils import call_model

# Questions de test vari√©es
TEST_QUESTIONS = [
    "Salut !",
    "C'est quoi Python ?",
    "Parle-moi de Django",
    "tu connais Minecraft ?",
    "Explique les tests unitaires",
    "Comment faire une API REST ?",
    "C'est quoi Git ?",
    "Pourquoi utiliser TypeScript ?",
]


@pytest.fixture(scope="module")
def config():
    """Load config once for all tests."""
    return load_config()


@pytest.mark.llm
@pytest.mark.asyncio
async def test_model_warmup(config):
    """Warmup du mod√®le avec 2 requ√™tes simples."""
    print("\nüî• Warmup du mod√®le...")
    
    for i in range(2):
        unique_user = f"warmup_{i}"
        prompt = make_prompt(mode="chill", content="test", user=unique_user)
        response = await call_model(prompt, config, user=unique_user, mode="chill")
        assert response is not None, f"Warmup {i+1}/2 failed"
        print(f"   ‚úÖ Warmup {i+1}/2: {len(response)} chars")
    
    print("   üî• Mod√®le pr√™t !\n")


@pytest.mark.llm
@pytest.mark.asyncio
async def test_model_coherence(config):
    """
    Test que le mod√®le g√©n√®re des r√©ponses coh√©rentes et pertinentes.
    
    V√©rifie:
    - Phrases compl√®tes (finissent par . ! ? ou ```)
    - Longueur raisonnable (< 300 chars pour Twitch)
    - Coh√©rence basique (pas de mots absurdes)
    """
    print("\nüß™ Test de coh√©rence sur 8 questions")
    print("=" * 70)
    
    results = []
    coherent_count = 0
    complete_count = 0
    
    for i, question in enumerate(TEST_QUESTIONS, 1):
        print(f"\n‚ùì Q{i}: {question}")
        
        unique_user = f"test_coherence_{i}"
        prompt = make_prompt(mode="chill", content=question, user=unique_user)
        response = await call_model(prompt, config, user=unique_user, mode="chill")
        
        assert response is not None, f"Question {i} returned None"
        
        # Analyse de la r√©ponse
        char_count = len(response)
        is_complete = response.rstrip().endswith((".", "!", "?", "```"))
        
        # D√©tection de mots absurdes (heuristique simple)
        nonsense_words = ["cyborg", "√©toile du", "jambes solides", "botte cyber"]
        has_nonsense = any(word in response.lower() for word in nonsense_words)
        
        is_coherent = is_complete and not has_nonsense and char_count < 300
        
        if is_complete:
            complete_count += 1
        if is_coherent:
            coherent_count += 1
        
        status = "‚úÖ" if is_coherent else "‚ö†Ô∏è"
        print(f"{status} {char_count} chars: {response[:100]}...")
        
        results.append({
            "question": question,
            "response": response,
            "chars": char_count,
            "complete": is_complete,
            "coherent": is_coherent
        })
    
    # Assertions
    print("\n" + "=" * 70)
    print("üìä R√âSULTATS:")
    print(f"   Phrases compl√®tes: {complete_count}/{len(TEST_QUESTIONS)} ({complete_count/len(TEST_QUESTIONS)*100:.0f}%)")
    print(f"   R√©ponses coh√©rentes: {coherent_count}/{len(TEST_QUESTIONS)} ({coherent_count/len(TEST_QUESTIONS)*100:.0f}%)")
    
    avg_chars = sum(r['chars'] for r in results) / len(results)
    print(f"   Longueur moyenne: {avg_chars:.0f} chars")
    print("=" * 70)
    
    # Au moins 80% de phrases compl√®tes
    assert complete_count >= len(TEST_QUESTIONS) * 0.8, \
        f"Trop de phrases tronqu√©es: {complete_count}/{len(TEST_QUESTIONS)}"
    
    # Au moins 75% de r√©ponses coh√©rentes
    assert coherent_count >= len(TEST_QUESTIONS) * 0.75, \
        f"Trop de r√©ponses incoh√©rentes: {coherent_count}/{len(TEST_QUESTIONS)}"
    
    # Longueur moyenne raisonnable pour Twitch
    assert avg_chars < 300, f"R√©ponses trop longues: {avg_chars:.0f} chars (max 300)"
    
    print("\n‚úÖ Test de coh√©rence r√©ussi !")


@pytest.mark.llm
@pytest.mark.asyncio
async def test_model_response_time(config):
    """
    Test que le mod√®le r√©pond dans des d√©lais acceptables.
    
    Target: < 3s par r√©ponse pour exp√©rience Twitch fluide
    """
    print("\n‚ö° Test de latence")
    print("=" * 70)
    
    import time
    
    durations = []
    
    # Test sur 5 questions vari√©es
    test_questions = TEST_QUESTIONS[:5]
    
    for idx, question in enumerate(test_questions):
        unique_user = f"test_latency_{idx}"
        prompt = make_prompt(mode="chill", content=question, user=unique_user)
        
        start = time.time()
        response = await call_model(prompt, config, user=unique_user, mode="chill")
        duration = time.time() - start
        
        assert response is not None, f"Question '{question}' returned None"
        
        durations.append(duration)
        status = "‚úÖ" if duration < 3.0 else "‚ö†Ô∏è"
        print(f"{status} {question:30s} ‚Üí {duration:.2f}s")
    
    avg_duration = sum(durations) / len(durations)
    max_duration = max(durations)
    
    print("=" * 70)
    print(f"üìä Latence moyenne: {avg_duration:.2f}s")
    print(f"üìä Latence max: {max_duration:.2f}s")
    print("=" * 70)
    
    # Latence moyenne acceptable
    assert avg_duration < 3.0, f"Latence trop √©lev√©e: {avg_duration:.2f}s (max 3s)"
    
    print("\n‚úÖ Test de latence r√©ussi !")


@pytest.mark.llm
@pytest.mark.asyncio
async def test_model_conciseness(config):
    """
    Test que le mod√®le respecte la contrainte de concision.
    
    On tol√®re les d√©passements sur questions complexes (tests unitaires, API, etc.)
    Chaque question utilise un user_id unique pour √©viter l'accumulation de contexte.
    """
    print("\nüìè Test de concision")
    print("=" * 70)
    
    results = []
    latencies = []
    
    for idx, question in enumerate(TEST_QUESTIONS):
        start_time = time.time()
        # User ID unique pour chaque question = pas de contexte entre questions
        unique_user = f"test_concision_{idx}"
        prompt = make_prompt(mode="chill", content=question, user=unique_user)
        response = await call_model(prompt, config, user=unique_user, mode="chill")
        latency = time.time() - start_time
        
        assert response is not None, f"Question '{question}' returned None"
        
        char_count = len(response)
        results.append(char_count)
        latencies.append(latency)
        
        status = "‚úÖ" if char_count <= 150 else "‚ö†Ô∏è" if char_count <= 400 else "‚ùå"
        print(f"{status} {question:30s} ‚Üí {char_count} chars, {latency:.2f}s")
    
    avg_chars = sum(results) / len(results)
    avg_latency = sum(latencies) / len(latencies)
    within_150 = sum(1 for c in results if c <= 150)
    within_400 = sum(1 for c in results if c <= 400)
    over_500 = sum(1 for c in results if c > 500)
    
    print("=" * 70)
    print(f"üìä Moyenne: {avg_chars:.0f} chars, {avg_latency:.2f}s")
    print(f"üìä ‚â§150 chars: {within_150}/{len(results)} ({within_150/len(results)*100:.0f}%)")
    print(f"üìä ‚â§400 chars: {within_400}/{len(results)} ({within_400/len(results)*100:.0f}%)")
    print("=" * 70)
    
    # Au moins 50% des r√©ponses doivent √™tre ‚â§150 chars (concises)
    assert within_150 >= len(results) * 0.5, \
        f"Pas assez de r√©ponses concises: {within_150}/{len(results)}"
    
    # Moyenne raisonnable pour Twitch (< 200 chars)
    assert avg_chars < 200, \
        f"Moyenne trop √©lev√©e: {avg_chars:.0f} chars (max 200)"
    
    # Aucune r√©ponse ne doit d√©passer 500 chars (limite Twitch)
    assert over_500 == 0, \
        f"R√©ponses d√©passent limite Twitch: {over_500}/{len(results)} > 500 chars"
    
    print("\n‚úÖ Test de concision r√©ussi !")
