"""
Intelligence Events - @mention handler
"""

from intelligence.core import process_llm_request, extract_mention_message


async def handle_mention(bot, message):
    """
    Traite les @mention du bot dans le chat.
    
    Args:
        bot: Instance du bot
        message: Message TwitchIO
    
    Returns:
        str: RÃ©ponse gÃ©nÃ©rÃ©e ou None
    """
    # ğŸ“¦ Extraction message
    bot.logger.info(f"ğŸ” Extraction: message='{message.content}', bot_name='{bot.nick}'")
    user_message = extract_mention_message(message.content, bot.nick)
    bot.logger.info(f"ğŸ” User message extrait: '{user_message}'")
    
    if not user_message:
        bot.logger.warning(f"âš ï¸ Extraction Ã©chouÃ©e ! Message vide aprÃ¨s extraction.")
        return None
    
    # â±ï¸ Rate limit check dÃ©jÃ  fait dans bot.py - pas besoin ici
    # if not bot.rate_limiter.is_allowed(message.author.name, cooldown=15.0):
    #     remaining = bot.rate_limiter.get_remaining_cooldown(message.author.name, cooldown=15.0)
    #     return f"@{message.author.name} Cooldown! Attends {remaining:.1f}s"
    
    # ğŸ§  Logique mÃ©tier (testable) - RÃ©cupÃ©rer LLMHandler partagÃ©
    bot.logger.info(f"ğŸ” Cogs disponibles: {list(bot.cogs.keys())}")
    intelligence_cog = bot.get_cog('IntelligenceCommands')
    bot.logger.info(f"ğŸ” Intelligence Cog trouvÃ©: {intelligence_cog is not None}")
    
    if not intelligence_cog:
        bot.logger.error("IntelligenceCommands Cog not loaded!")
        return f"@{message.author.name} Erreur IA ğŸ˜µ"
    
    response = await process_llm_request(
        llm_handler=intelligence_cog.llm_handler,
        prompt=user_message,
        context="mention",
        user_name=message.author.name
    )
    
    # ğŸ’¬ RÃ©ponse Twitch
    if not response:
        return f"@{message.author.name} Erreur IA ğŸ˜µ"
    
    return f"@{message.author.name} {response}"
