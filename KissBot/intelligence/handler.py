"""
KissBot V1 - LLM Handler KISS avec cascade fallback + Model-Specific Prompting
Version simplifi√©e inspir√©e de SerdaBot avec randomizer El_Serda + optimisation prompts !
"""

import logging
import random
import httpx
from typing import Dict, Optional


class ModelPromptOptimizer:
    """Optimiseur de prompts par mod√®le - Version KISS."""
    
    @staticmethod
    def get_system_prompt(model_type: str, context: str, bot_name: str) -> str:
        """Retourne le prompt syst√®me optimis√© par mod√®le."""
        import logging
        logger = logging.getLogger(__name__)
        
        # D√©tection mod√®le simple
        model_lower = model_type.lower()
        logger.info(f"üß† ModelPromptOptimizer: D√©tection mod√®le '{model_type}' -> '{model_lower}'")
        
        if "qwen" in model_lower:
            prompt = f"Tu es {bot_name}, bot Twitch. R√©ponds en fran√ßais, factuel et concis, max 150 chars. {context}"
            logger.info(f"üéØ Prompt QWEN optimis√© s√©lectionn√©")
            return prompt
            
        elif "llama" in model_lower:
            prompt = f"Tu es {bot_name}, assistant gaming sur Twitch. Sois concis et utile. {context}"
            logger.info(f"ü¶ô Prompt LLAMA optimis√© s√©lectionn√©")
            return prompt
            
        elif "gpt" in model_lower or "openai" in model_lower:
            prompt = f"You are {bot_name}, a gaming expert on Twitch. Be helpful and concise. Context: {context}"
            logger.info(f"ü§ñ Prompt OPENAI optimis√© s√©lectionn√©")
            return prompt
            
        else:
            # Fallback g√©n√©rique
            prompt = f"Tu es {bot_name}, bot Twitch gaming. R√©ponds en fran√ßais, max 150 chars. {context}"
            logger.info(f"üîß Prompt GENERIQUE utilis√© (fallback)")
            return prompt


class LLMHandler:
    """Handler LLM KISS avec cascade √† 3 niveaux."""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        apis_config = config.get('apis', {})
        llm_config = config.get('llm', {})
        
        # Configuration avec option local_llm ON/OFF
        self.provider = llm_config.get('provider', 'local')
        self.local_llm_enabled = llm_config.get('local_llm', True)  # üî• ON/OFF pour LM Studio
        self.local_endpoint = llm_config.get('model_endpoint') if self.local_llm_enabled else None
        self.local_model = llm_config.get('model_name', 'qwen2.5-7b-instruct')
        self.openai_key = apis_config.get('openai_key')
        self.openai_model = llm_config.get('openai_model', 'gpt-3.5-turbo')
        
        # Param√®tres
        self.max_tokens_ask = llm_config.get('max_tokens_ask', 150)
        self.max_tokens_mention = llm_config.get('max_tokens_mention', 80)
        self.temperature_ask = llm_config.get('temperature_ask', 0.7)
        self.temperature_mention = llm_config.get('temperature_mention', 0.9)
        
        # üé≠ Bot personality from config (fallback)
        self.bot_name = config.get('bot', {}).get('name', 'UnknownBot')  # Fallback avant TwitchIO
        self.personality = config.get('bot', {}).get('personality', 'sympa, direct, et passionn√© de tech')
        
        # üéØ Flag: Utiliser personality selon contexte ET mod√®le
        # ask/command = CLEAN (factuel)
        # mention/chill = PERSONALITY
        self.use_personality_on_mention = llm_config.get('use_personality_on_mention', True)
        self.use_personality_on_ask = llm_config.get('use_personality_on_ask', False)
        self.personality_only_on_cloud = llm_config.get('personality_only_on_cloud', True)  # üí° Genius mode
        

        
        # Cache des endpoints √©chou√©s (style SerdaBot mais simple)
        self.failed_endpoints: set[str] = set()
        
        # Validation et logs
        self.enabled = bool(self.local_endpoint or self.openai_key)
        if self.enabled:
            local_status = "‚úÖ ON" if self.local_llm_enabled else "‚ùå OFF"
            openai_status = "‚úÖ" if self.openai_key else "‚ùå"
            self.logger.info(f"ü§ñ LLM configur√© - Local: {local_status} | OpenAI: {openai_status} | Provider: {self.provider}")

        else:
            self.logger.warning("‚ö†Ô∏è Aucun LLM configur√© - Mode r√©pliques fun uniquement")
    
    def update_bot_name(self, twitch_name: str) -> None:
        """üîÑ Met √† jour le nom du bot avec le vrai nom TwitchIO."""
        old_name = self.bot_name
        self.bot_name = twitch_name
        self.logger.info(f"üè∑Ô∏è Bot name updated: '{old_name}' ‚Üí '{twitch_name}'")
    
    async def generate_response(self, prompt: str, context: str = "general", user_name: str = "") -> Optional[str]:
        """G√©n√®re une r√©ponse LLM avec fallback cascade."""
        
        # Si LLM d√©sactiv√© ‚Üí fallback
        if not self.enabled:
            return self.get_fallback_response(context, user_name)
        
        # Param√®tres selon contexte
        if context == "ask":
            max_tokens = self.max_tokens_ask
            temperature = self.temperature_ask
        else:
            max_tokens = self.max_tokens_mention
            temperature = self.temperature_mention
        
        # üéØ OPTIMISATION MODEL-SPECIFIC PROMPTING
        model_type = self._detect_model_type()
        system_prompt = ModelPromptOptimizer.get_system_prompt(
            model_type, context, self.bot_name
        )
        self.logger.info(f"üöÄ Optimisation activ√©e: mod√®le={model_type}, context={context}")
        
        # Tentative local en premier
        if self.local_llm_enabled:
            response = await self._try_local(prompt, system_prompt, max_tokens, temperature)
            if response:
                return response
            self.logger.warning("LLM local √©chou√©, tentative OpenAI...")
        
        # Fallback OpenAI
        if self.openai_key:
            response = await self._try_openai(prompt, system_prompt, max_tokens, temperature)
            if response:
                return response
            self.logger.warning("OpenAI √©chou√©, fallback statique...")
        
        # Fallback statique final
        return self.get_fallback_response(context, user_name)
    async def _try_openai(self, prompt: str, system_prompt: str, max_tokens: int, temperature: float) -> Optional[str]:
        """Essai OpenAI avec gestion d'erreur et quota."""
        try:
            payload = {
                "model": self.openai_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            headers = {
                "Authorization": f"Bearer {self.openai_key}",
                "Content-Type": "application/json"
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post("https://api.openai.com/v1/chat/completions", 
                                           json=payload, headers=headers)
                response.raise_for_status()
                
                data = response.json()
                if "choices" in data and data["choices"]:
                    return data["choices"][0]["message"]["content"].strip()
                
        except Exception as e:
            self.logger.error(f"üí• ERREUR OPENAI D√âTAILL√âE: {e}")
            # Gestion sp√©ciale quota/rate limit
            error_str = str(e).lower()
            if any(word in error_str for word in ['quota', 'rate limit', 'billing', 'insufficient']):
                self.logger.warning("üö® OpenAI quota/rate limit atteint!")
        
        return None
    
    async def _check_local_health(self) -> bool:
        """üè• Health check rapide du LLM local (2s max)."""
        try:
            base_url = self.local_endpoint.replace('/chat/completions', '/models') if self.local_endpoint else "http://127.0.0.1:1234/v1/models"
            async with httpx.AsyncClient(timeout=2.0) as client:
                response = await client.get(base_url)
                return response.status_code == 200
        except Exception:
            return False

    async def _try_local(self, prompt: str, system_prompt: str, max_tokens: int, temperature: float) -> Optional[str]:
        """Essai Local LM Studio avec health check pr√©alable."""
        if not self.local_llm_enabled:
            self.logger.debug("üîá Local LLM d√©sactiv√© via config")
            return None
        
        # üè• Health check rapide d'abord
        if not await self._check_local_health():
            self.logger.warning("üíÄ Local LLM health check √©chou√© - skip direct")
            # Cache temporaire d√©sactiv√© pour permettre retry
            # self.failed_endpoints.add("local")
            return None
            
        try:
            payload = {
                "model": self.local_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(self.local_endpoint, json=payload)
                response.raise_for_status()
                
                data = response.json()
                if "choices" in data and data["choices"]:
                    raw_response = data["choices"][0]["message"]["content"]
                    cleaned_response = raw_response.strip() if raw_response else ""
                    
                    if cleaned_response:
                        # Succ√®s ‚Üí retirer du cache d'√©chec (style SerdaBot)
                        self.failed_endpoints.discard("local")
                        return cleaned_response
                    else:
                        self.logger.warning("‚ö†Ô∏è LLM a retourn√© une r√©ponse vide!")
                        return None
                
        except Exception as e:
            self.logger.error(f"Local LLM error: {e}")
            # Note: Cache d'√©chec non impl√©ment√© par choix KISS
        
        return None
    
    def _get_fun_fallback(self, context: str, user_name: str = "") -> str:
        """R√©pliques de fallback simples."""
        
        fallbacks = {
            "ask": [
                "ü§î Je r√©fl√©chis... üí≠",
                "Mon cerveau est en pause caf√© ‚òï",
                "Question int√©ressante ! Mais mon IA fait la sieste üò¥",
                "D√©sol√©, mes neurones sont en mode √©conomie d'√©nergie üîã",
                "Sans mon LLM, je suis juste un bot qui dit des trucs random ü§∑"
            ],
            "mention": [
                "Hey ! Tu m'appelles ? üìû", 
                "Pr√©sent ! üôã‚Äç‚ôÇÔ∏è", 
                "Ouais ? üòè", 
                "Tu veux quoi chef ? üë®‚Äçüíº", 
                "J'√©coute ! üëÇ"
            ],
            "general": [
                "ü§ñ Bip boop, je suis l√† !", 
                "Mode robot activ√© ! üöÄ", 
                "Pr√™t √† vous servir ! ü´°", 
                "En ligne ! ‚ö°"
            ]
        }
        
        responses = fallbacks.get(context, fallbacks["general"])
        return random.choice(responses)
    
    def _detect_model_type(self) -> str:
        """D√©tecte le type de mod√®le en cours d'utilisation - Version KISS."""
        if self.local_llm_enabled and self.local_model:
            self.logger.info(f"üîç Mod√®le d√©tect√©: LOCAL -> '{self.local_model}'")
            return self.local_model
        elif self.openai_key and self.openai_model:
            self.logger.info(f"üîç Mod√®le d√©tect√©: OPENAI -> '{self.openai_model}'")
            return self.openai_model
        else:
            self.logger.info(f"üîç Mod√®le d√©tect√©: DEFAULT (aucun mod√®le configur√©)")
            return "default"

    def get_fallback_response(self, context: str, user_name: str = "") -> str:
        """Fallback statique simple."""
        fallbacks = {
            "ask": "D√©sol√©, mon IA n'est pas disponible. Essayez plus tard ! ü§ñ",
            "mention": "Salut ! Mon IA fait une pause mais je suis l√† ! üëã",
            "general": "Hmm, int√©ressant... ü§î"
        }
        return fallbacks.get(context, fallbacks["general"])
