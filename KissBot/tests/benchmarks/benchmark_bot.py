#!/usr/bin/env python3
"""
ðŸ”¥ KISSBOT FULL BENCHMARK
Teste performance bot + LLM + cache + cascade
"""

import asyncio
import time
import statistics
import sys
import os
from typing import List, Dict, Any
import json

# Setup path pour imports KissBot
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from intelligence.handler import LLMHandler
    from config_loader import load_config
    import yaml
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("ðŸ’¡ Make sure you're running from KissBot/test/ directory")
    print("ðŸ’¡ And that local_llm.py exists in parent directory")
    sys.exit(1)

class KissBotBenchmark:
    def __init__(self):
        """Initialize benchmark with config."""
        config = load_config()
        self.config = config
        
        self.llm = LLMHandler(config)
        self.results = {}
        
        print("ðŸ”¥ KISSBOT BENCHMARK INITIALIZED")
        print(f"Local LLM: {'âœ…' if self.llm.local_llm_enabled else 'âŒ'}")
        print(f"OpenAI: {'âœ…' if self.llm.openai_key else 'âŒ'}")
        print("-" * 50)

    async def benchmark_llm_endpoints(self) -> Dict[str, Any]:
        """Benchmark LLM endpoints performance."""
        print("ðŸ§  BENCHMARKING LLM ENDPOINTS...")
        
        test_prompts = [
            "python",
            "javascript", 
            "c++",
            "rust",
            "machine learning"
        ]
        
        results = {
            "local": {"times": [], "success": 0, "errors": 0},
            "openai": {"times": [], "success": 0, "errors": 0}
        }
        
        # Test Local LLM
        if self.llm.local_llm_enabled:
            print("ðŸ”¬ Testing Local LLM...")
            for prompt in test_prompts:
                try:
                    start = time.time()
                    response = await self.llm._try_local(
                        prompt, 
                        "RÃ©ponds en franÃ§ais, concis.", 
                        200, 
                        0.5
                    )
                    end = time.time()
                    
                    if response:
                        results["local"]["times"].append(end - start)
                        results["local"]["success"] += 1
                        print(f"  âœ… {prompt}: {end-start:.2f}s - {len(response)} chars")
                    else:
                        results["local"]["errors"] += 1
                        print(f"  âŒ {prompt}: No response")
                        
                except Exception as e:
                    results["local"]["errors"] += 1
                    print(f"  ðŸ’¥ {prompt}: {e}")
                    
                await asyncio.sleep(0.5)  # Rate limiting
        
        # Test OpenAI
        if self.llm.openai_key:
            print("ðŸ”¬ Testing OpenAI...")
            for prompt in test_prompts:
                try:
                    start = time.time()
                    response = await self.llm._try_openai(
                        prompt,
                        "RÃ©ponds en franÃ§ais, concis.",
                        200,
                        0.5
                    )
                    end = time.time()
                    
                    if response:
                        results["openai"]["times"].append(end - start)
                        results["openai"]["success"] += 1
                        print(f"  âœ… {prompt}: {end-start:.2f}s - {len(response)} chars")
                    else:
                        results["openai"]["errors"] += 1
                        print(f"  âŒ {prompt}: No response")
                        
                except Exception as e:
                    results["openai"]["errors"] += 1
                    print(f"  ðŸ’¥ {prompt}: {e}")
                    
                await asyncio.sleep(0.5)
        
        return results

    async def benchmark_cascade_fallback(self) -> Dict[str, Any]:
        """Test cascade fallback performance."""
        print("\nðŸ”„ BENCHMARKING CASCADE FALLBACK...")
        
        test_cases = [
            {"prompt": "golang", "context": "ask"},
            {"prompt": "docker", "context": "ask"}, 
            {"prompt": "kubernetes", "context": "mention"},
            {"prompt": "react", "context": "chill"},
        ]
        
        results = {"times": [], "fallback_triggered": 0}
        
        for case in test_cases:
            try:
                start = time.time()
                response = await self.llm.generate_response(
                    case["prompt"],
                    context=case["context"],
                    user_name="benchmark_user"
                )
                end = time.time()
                
                duration = end - start
                results["times"].append(duration)
                
                if duration > 15:  # Si > 15s, probablement fallback
                    results["fallback_triggered"] += 1
                    
                print(f"  âœ… {case['prompt']} ({case['context']}): {duration:.2f}s")
                if response:
                    print(f"    ðŸ“ Response: {response[:100]}...")
                
            except Exception as e:
                print(f"  ðŸ’¥ {case['prompt']}: {e}")
                
            await asyncio.sleep(1)
        
        return results

    async def benchmark_context_switching(self) -> Dict[str, Any]:
        """Test different context performance."""
        print("\nðŸŽ­ BENCHMARKING CONTEXT SWITCHING...")
        
        contexts = ["ask", "mention", "chill"]
        prompt = "python web development"
        results = {}
        
        for context in contexts:
            times = []
            responses = []
            
            print(f"ðŸ”¬ Testing context: {context}")
            
            for i in range(3):  # 3 tests per context
                try:
                    start = time.time()
                    response = await self.llm.generate_response(
                        prompt,
                        context=context,
                        user_name=f"test_user_{i}"
                    )
                    end = time.time()
                    
                    duration = end - start
                    times.append(duration)
                    responses.append(response[:50] if response else "No response")
                    
                    print(f"  âœ… Test {i+1}: {duration:.2f}s")
                    
                except Exception as e:
                    print(f"  ðŸ’¥ Test {i+1}: {e}")
                    
                await asyncio.sleep(0.5)
            
            results[context] = {
                "times": times,
                "avg_time": statistics.mean(times) if times else 0,
                "responses": responses
            }
        
        return results

    async def benchmark_stress_test(self) -> Dict[str, Any]:
        """Stress test with concurrent requests."""
        print("\nâš¡ STRESS TEST - CONCURRENT REQUESTS...")
        
        async def single_request(prompt: str, context: str) -> float:
            start = time.time()
            try:
                response = await self.llm.generate_response(
                    prompt, context=context, user_name="stress_user"
                )
                return time.time() - start
            except:
                return -1  # Error
        
        # Test 5 concurrent requests
        tasks = []
        prompts = ["python", "javascript", "golang", "rust", "kotlin"]
        
        print("ðŸ”¬ Launching 5 concurrent requests...")
        start_total = time.time()
        
        for i, prompt in enumerate(prompts):
            task = single_request(prompt, "ask")
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        end_total = time.time()
        
        successful = [r for r in results if r > 0]
        failed = len([r for r in results if r < 0])
        
        return {
            "total_time": end_total - start_total,
            "individual_times": successful,
            "avg_time": statistics.mean(successful) if successful else 0,
            "success_rate": len(successful) / len(results) * 100,
            "failed_requests": failed
        }

    def generate_report(self) -> str:
        """Generate comprehensive benchmark report."""
        report = "\n" + "="*60 + "\n"
        report += "ðŸ”¥ KISSBOT BENCHMARK REPORT\n"
        report += "="*60 + "\n"
        
        # LLM Endpoints
        if "llm_endpoints" in self.results:
            report += "\nðŸ§  LLM ENDPOINTS PERFORMANCE:\n"
            for endpoint, data in self.results["llm_endpoints"].items():
                if data["times"]:
                    avg_time = statistics.mean(data["times"])
                    min_time = min(data["times"])
                    max_time = max(data["times"])
                    
                    report += f"\n{endpoint.upper()}:\n"
                    report += f"  â€¢ Average: {avg_time:.2f}s\n"
                    report += f"  â€¢ Min: {min_time:.2f}s\n"
                    report += f"  â€¢ Max: {max_time:.2f}s\n"
                    report += f"  â€¢ Success: {data['success']}/{data['success']+data['errors']}\n"
                    report += f"  â€¢ Success Rate: {data['success']/(data['success']+data['errors'])*100:.1f}%\n"
        
        # Cascade Fallback
        if "cascade" in self.results:
            data = self.results["cascade"]
            if data["times"]:
                report += f"\nðŸ”„ CASCADE FALLBACK:\n"
                report += f"  â€¢ Average Response Time: {statistics.mean(data['times']):.2f}s\n"
                report += f"  â€¢ Fallback Triggered: {data['fallback_triggered']} times\n"
        
        # Context Switching
        if "context" in self.results:
            report += f"\nðŸŽ­ CONTEXT SWITCHING:\n"
            for context, data in self.results["context"].items():
                report += f"  â€¢ {context.upper()}: {data['avg_time']:.2f}s avg\n"
        
        # Stress Test
        if "stress" in self.results:
            data = self.results["stress"]
            report += f"\nâš¡ STRESS TEST (5 concurrent):\n"
            report += f"  â€¢ Total Time: {data['total_time']:.2f}s\n"
            report += f"  â€¢ Average per Request: {data['avg_time']:.2f}s\n"
            report += f"  â€¢ Success Rate: {data['success_rate']:.1f}%\n"
            report += f"  â€¢ Failed Requests: {data['failed_requests']}\n"
        
        report += "\n" + "="*60 + "\n"
        return report

    async def run_full_benchmark(self):
        """Run complete benchmark suite."""
        print("ðŸš€ STARTING FULL KISSBOT BENCHMARK")
        print("â˜• Grab your tisane, this will take a moment...\n")
        
        try:
            # LLM Endpoints
            self.results["llm_endpoints"] = await self.benchmark_llm_endpoints()
            
            # Cascade Fallback
            self.results["cascade"] = await self.benchmark_cascade_fallback()
            
            # Context Switching
            self.results["context"] = await self.benchmark_context_switching()
            
            # Stress Test
            self.results["stress"] = await self.benchmark_stress_test()
            
            # Generate and save report
            report = self.generate_report()
            print(report)
            
            # Save to file
            with open("benchmark_results.json", "w") as f:
                json.dump(self.results, f, indent=2)
            
            with open("benchmark_report.txt", "w") as f:
                f.write(report)
            
            print("ðŸ“Š Results saved to benchmark_results.json and benchmark_report.txt")
            
        except Exception as e:
            print(f"ðŸ’¥ Benchmark failed: {e}")

async def main():
    """Main benchmark entry point."""
    benchmark = KissBotBenchmark()
    await benchmark.run_full_benchmark()

if __name__ == "__main__":
    asyncio.run(main())